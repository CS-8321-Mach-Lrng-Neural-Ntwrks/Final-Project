{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369fed9-9745-439f-bbbf-26555dcfce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 12:11:25,562 - INFO - Using device: cuda\n",
      "2025-05-10 12:11:26,172 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 16, 'learning_rate': 0.001, 'window_size': 64}\n",
      "2025-05-10 12:11:29,011 - INFO - Epoch 1/100 | Train Loss: 0.913502 | Val Loss: 0.786424\n",
      "2025-05-10 12:11:29,462 - INFO - Epoch 2/100 | Train Loss: 0.663425 | Val Loss: 0.576862\n",
      "2025-05-10 12:11:30,009 - INFO - Epoch 3/100 | Train Loss: 0.519312 | Val Loss: 0.487597\n",
      "2025-05-10 12:11:30,471 - INFO - Epoch 4/100 | Train Loss: 0.443991 | Val Loss: 0.412525\n",
      "2025-05-10 12:11:30,929 - INFO - Epoch 5/100 | Train Loss: 0.390275 | Val Loss: 0.364140\n",
      "2025-05-10 12:11:31,344 - INFO - Epoch 6/100 | Train Loss: 0.350962 | Val Loss: 0.340567\n",
      "2025-05-10 12:11:31,758 - INFO - Epoch 7/100 | Train Loss: 0.320169 | Val Loss: 0.302712\n",
      "2025-05-10 12:11:32,220 - INFO - Epoch 8/100 | Train Loss: 0.302282 | Val Loss: 0.304831\n",
      "2025-05-10 12:11:32,629 - INFO - Epoch 9/100 | Train Loss: 0.286440 | Val Loss: 0.270625\n",
      "2025-05-10 12:11:33,086 - INFO - Epoch 10/100 | Train Loss: 0.270611 | Val Loss: 0.258163\n",
      "2025-05-10 12:11:33,553 - INFO - Epoch 11/100 | Train Loss: 0.254325 | Val Loss: 0.252386\n",
      "2025-05-10 12:11:33,982 - INFO - Epoch 12/100 | Train Loss: 0.245461 | Val Loss: 0.244738\n",
      "2025-05-10 12:11:34,424 - INFO - Epoch 13/100 | Train Loss: 0.233227 | Val Loss: 0.227821\n",
      "2025-05-10 12:11:34,838 - INFO - Epoch 14/100 | Train Loss: 0.240493 | Val Loss: 0.222084\n",
      "2025-05-10 12:11:35,286 - INFO - Epoch 15/100 | Train Loss: 0.222024 | Val Loss: 0.211314\n",
      "2025-05-10 12:11:35,739 - INFO - Epoch 16/100 | Train Loss: 0.224790 | Val Loss: 0.213449\n",
      "2025-05-10 12:11:36,190 - INFO - Epoch 17/100 | Train Loss: 0.210284 | Val Loss: 0.213901\n",
      "2025-05-10 12:11:36,635 - INFO - Epoch 18/100 | Train Loss: 0.206372 | Val Loss: 0.206969\n",
      "2025-05-10 12:11:37,088 - INFO - Epoch 19/100 | Train Loss: 0.214844 | Val Loss: 0.217274\n",
      "2025-05-10 12:11:37,531 - INFO - Epoch 20/100 | Train Loss: 0.200745 | Val Loss: 0.207027\n",
      "2025-05-10 12:11:38,006 - INFO - Epoch 21/100 | Train Loss: 0.210970 | Val Loss: 0.206981\n",
      "2025-05-10 12:11:38,461 - INFO - Epoch 22/100 | Train Loss: 0.203082 | Val Loss: 0.218378\n",
      "2025-05-10 12:11:38,919 - INFO - Epoch 23/100 | Train Loss: 0.203303 | Val Loss: 0.219202\n",
      "2025-05-10 12:11:39,371 - INFO - Epoch 24/100 | Train Loss: 0.206089 | Val Loss: 0.194544\n",
      "2025-05-10 12:11:39,786 - INFO - Epoch 25/100 | Train Loss: 0.206040 | Val Loss: 0.177000\n",
      "2025-05-10 12:11:40,270 - INFO - Epoch 26/100 | Train Loss: 0.202075 | Val Loss: 0.207312\n",
      "2025-05-10 12:11:40,737 - INFO - Epoch 27/100 | Train Loss: 0.192825 | Val Loss: 0.196546\n",
      "2025-05-10 12:11:41,171 - INFO - Epoch 28/100 | Train Loss: 0.195237 | Val Loss: 0.196451\n",
      "2025-05-10 12:11:41,626 - INFO - Epoch 29/100 | Train Loss: 0.191322 | Val Loss: 0.198981\n",
      "2025-05-10 12:11:42,073 - INFO - Epoch 30/100 | Train Loss: 0.188944 | Val Loss: 0.179938\n",
      "2025-05-10 12:11:42,542 - INFO - Epoch 31/100 | Train Loss: 0.188486 | Val Loss: 0.182228\n",
      "2025-05-10 12:11:42,984 - INFO - Epoch 32/100 | Train Loss: 0.192933 | Val Loss: 0.175696\n",
      "2025-05-10 12:11:43,428 - INFO - Epoch 33/100 | Train Loss: 0.192595 | Val Loss: 0.207298\n",
      "2025-05-10 12:11:43,863 - INFO - Epoch 34/100 | Train Loss: 0.182584 | Val Loss: 0.183304\n",
      "2025-05-10 12:11:44,376 - INFO - Epoch 35/100 | Train Loss: 0.187390 | Val Loss: 0.192942\n",
      "2025-05-10 12:11:44,916 - INFO - Epoch 36/100 | Train Loss: 0.189826 | Val Loss: 0.183975\n",
      "2025-05-10 12:11:45,451 - INFO - Epoch 37/100 | Train Loss: 0.184974 | Val Loss: 0.183264\n",
      "2025-05-10 12:11:45,913 - INFO - Epoch 38/100 | Train Loss: 0.175923 | Val Loss: 0.173177\n",
      "2025-05-10 12:11:46,382 - INFO - Epoch 39/100 | Train Loss: 0.177650 | Val Loss: 0.186055\n",
      "2025-05-10 12:11:46,853 - INFO - Epoch 40/100 | Train Loss: 0.185906 | Val Loss: 0.175229\n",
      "2025-05-10 12:11:47,312 - INFO - Epoch 41/100 | Train Loss: 0.169739 | Val Loss: 0.163785\n",
      "2025-05-10 12:11:47,772 - INFO - Epoch 42/100 | Train Loss: 0.178502 | Val Loss: 0.167516\n",
      "2025-05-10 12:11:48,216 - INFO - Epoch 43/100 | Train Loss: 0.175539 | Val Loss: 0.173019\n",
      "2025-05-10 12:11:48,695 - INFO - Epoch 44/100 | Train Loss: 0.176371 | Val Loss: 0.162994\n",
      "2025-05-10 12:11:49,151 - INFO - Epoch 45/100 | Train Loss: 0.177342 | Val Loss: 0.174993\n",
      "2025-05-10 12:11:49,593 - INFO - Epoch 46/100 | Train Loss: 0.170685 | Val Loss: 0.173010\n",
      "2025-05-10 12:11:50,066 - INFO - Epoch 47/100 | Train Loss: 0.184564 | Val Loss: 0.175982\n",
      "2025-05-10 12:11:50,483 - INFO - Epoch 48/100 | Train Loss: 0.181891 | Val Loss: 0.163667\n",
      "2025-05-10 12:11:50,967 - INFO - Epoch 49/100 | Train Loss: 0.184159 | Val Loss: 0.170575\n",
      "2025-05-10 12:11:51,400 - INFO - Epoch 50/100 | Train Loss: 0.172812 | Val Loss: 0.191823\n",
      "2025-05-10 12:11:51,885 - INFO - Epoch 51/100 | Train Loss: 0.178775 | Val Loss: 0.176909\n",
      "2025-05-10 12:11:52,331 - INFO - Epoch 52/100 | Train Loss: 0.183297 | Val Loss: 0.165432\n",
      "2025-05-10 12:11:52,782 - INFO - Epoch 53/100 | Train Loss: 0.174912 | Val Loss: 0.180294\n",
      "2025-05-10 12:11:53,242 - INFO - Epoch 54/100 | Train Loss: 0.172011 | Val Loss: 0.181249\n",
      "2025-05-10 12:11:53,243 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:11:53,244 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 16, 'learning_rate': 0.001, 'window_size': 96}\n",
      "2025-05-10 12:11:53,686 - INFO - Epoch 1/100 | Train Loss: 0.948123 | Val Loss: 0.865622\n",
      "2025-05-10 12:11:54,135 - INFO - Epoch 2/100 | Train Loss: 0.773282 | Val Loss: 0.720250\n",
      "2025-05-10 12:11:54,580 - INFO - Epoch 3/100 | Train Loss: 0.675302 | Val Loss: 0.638131\n",
      "2025-05-10 12:11:55,066 - INFO - Epoch 4/100 | Train Loss: 0.615150 | Val Loss: 0.606829\n",
      "2025-05-10 12:11:55,518 - INFO - Epoch 5/100 | Train Loss: 0.575586 | Val Loss: 0.564937\n",
      "2025-05-10 12:11:55,971 - INFO - Epoch 6/100 | Train Loss: 0.554046 | Val Loss: 0.539432\n",
      "2025-05-10 12:11:56,489 - INFO - Epoch 7/100 | Train Loss: 0.527346 | Val Loss: 0.525223\n",
      "2025-05-10 12:11:56,949 - INFO - Epoch 8/100 | Train Loss: 0.514511 | Val Loss: 0.503862\n",
      "2025-05-10 12:11:57,425 - INFO - Epoch 9/100 | Train Loss: 0.500703 | Val Loss: 0.491702\n",
      "2025-05-10 12:11:57,893 - INFO - Epoch 10/100 | Train Loss: 0.492791 | Val Loss: 0.500701\n",
      "2025-05-10 12:11:58,366 - INFO - Epoch 11/100 | Train Loss: 0.489277 | Val Loss: 0.491743\n",
      "2025-05-10 12:11:58,809 - INFO - Epoch 12/100 | Train Loss: 0.490287 | Val Loss: 0.487493\n",
      "2025-05-10 12:11:59,255 - INFO - Epoch 13/100 | Train Loss: 0.478552 | Val Loss: 0.481125\n",
      "2025-05-10 12:11:59,740 - INFO - Epoch 14/100 | Train Loss: 0.475112 | Val Loss: 0.474927\n",
      "2025-05-10 12:12:00,232 - INFO - Epoch 15/100 | Train Loss: 0.472960 | Val Loss: 0.469244\n",
      "2025-05-10 12:12:00,810 - INFO - Epoch 16/100 | Train Loss: 0.474323 | Val Loss: 0.469845\n",
      "2025-05-10 12:12:01,347 - INFO - Epoch 17/100 | Train Loss: 0.469493 | Val Loss: 0.474538\n",
      "2025-05-10 12:12:01,849 - INFO - Epoch 18/100 | Train Loss: 0.469850 | Val Loss: 0.468393\n",
      "2025-05-10 12:12:02,385 - INFO - Epoch 19/100 | Train Loss: 0.465724 | Val Loss: 0.476364\n",
      "2025-05-10 12:12:02,855 - INFO - Epoch 20/100 | Train Loss: 0.467966 | Val Loss: 0.473692\n",
      "2025-05-10 12:12:03,353 - INFO - Epoch 21/100 | Train Loss: 0.461311 | Val Loss: 0.470756\n",
      "2025-05-10 12:12:03,821 - INFO - Epoch 22/100 | Train Loss: 0.472653 | Val Loss: 0.456900\n",
      "2025-05-10 12:12:04,325 - INFO - Epoch 23/100 | Train Loss: 0.471840 | Val Loss: 0.460843\n",
      "2025-05-10 12:12:04,809 - INFO - Epoch 24/100 | Train Loss: 0.471278 | Val Loss: 0.480389\n",
      "2025-05-10 12:12:05,288 - INFO - Epoch 25/100 | Train Loss: 0.469169 | Val Loss: 0.476026\n",
      "2025-05-10 12:12:05,726 - INFO - Epoch 26/100 | Train Loss: 0.466470 | Val Loss: 0.470290\n",
      "2025-05-10 12:12:06,176 - INFO - Epoch 27/100 | Train Loss: 0.469885 | Val Loss: 0.478079\n",
      "2025-05-10 12:12:06,649 - INFO - Epoch 28/100 | Train Loss: 0.466289 | Val Loss: 0.458810\n",
      "2025-05-10 12:12:07,136 - INFO - Epoch 29/100 | Train Loss: 0.464111 | Val Loss: 0.463528\n",
      "2025-05-10 12:12:07,627 - INFO - Epoch 30/100 | Train Loss: 0.471304 | Val Loss: 0.469995\n",
      "2025-05-10 12:12:08,094 - INFO - Epoch 31/100 | Train Loss: 0.470445 | Val Loss: 0.453570\n",
      "2025-05-10 12:12:08,547 - INFO - Epoch 32/100 | Train Loss: 0.461513 | Val Loss: 0.473372\n",
      "2025-05-10 12:12:09,056 - INFO - Epoch 33/100 | Train Loss: 0.463617 | Val Loss: 0.471101\n",
      "2025-05-10 12:12:09,511 - INFO - Epoch 34/100 | Train Loss: 0.469560 | Val Loss: 0.469381\n",
      "2025-05-10 12:12:09,961 - INFO - Epoch 35/100 | Train Loss: 0.465991 | Val Loss: 0.475518\n",
      "2025-05-10 12:12:10,414 - INFO - Epoch 36/100 | Train Loss: 0.466862 | Val Loss: 0.471300\n",
      "2025-05-10 12:12:10,863 - INFO - Epoch 37/100 | Train Loss: 0.463653 | Val Loss: 0.462297\n",
      "2025-05-10 12:12:11,343 - INFO - Epoch 38/100 | Train Loss: 0.461434 | Val Loss: 0.480727\n",
      "2025-05-10 12:12:11,856 - INFO - Epoch 39/100 | Train Loss: 0.467291 | Val Loss: 0.459880\n",
      "2025-05-10 12:12:12,328 - INFO - Epoch 40/100 | Train Loss: 0.461560 | Val Loss: 0.471931\n",
      "2025-05-10 12:12:12,824 - INFO - Epoch 41/100 | Train Loss: 0.472683 | Val Loss: 0.468567\n",
      "2025-05-10 12:12:12,825 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:12:12,825 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 16, 'learning_rate': 0.0005, 'window_size': 64}\n",
      "2025-05-10 12:12:13,277 - INFO - Epoch 1/100 | Train Loss: 0.964112 | Val Loss: 0.902261\n",
      "2025-05-10 12:12:13,723 - INFO - Epoch 2/100 | Train Loss: 0.808435 | Val Loss: 0.714524\n",
      "2025-05-10 12:12:14,134 - INFO - Epoch 3/100 | Train Loss: 0.659345 | Val Loss: 0.604965\n",
      "2025-05-10 12:12:14,634 - INFO - Epoch 4/100 | Train Loss: 0.570629 | Val Loss: 0.543945\n",
      "2025-05-10 12:12:15,088 - INFO - Epoch 5/100 | Train Loss: 0.496903 | Val Loss: 0.475556\n",
      "2025-05-10 12:12:15,527 - INFO - Epoch 6/100 | Train Loss: 0.447631 | Val Loss: 0.431783\n",
      "2025-05-10 12:12:15,979 - INFO - Epoch 7/100 | Train Loss: 0.412339 | Val Loss: 0.408691\n",
      "2025-05-10 12:12:16,513 - INFO - Epoch 8/100 | Train Loss: 0.381348 | Val Loss: 0.393703\n",
      "2025-05-10 12:12:17,044 - INFO - Epoch 9/100 | Train Loss: 0.357377 | Val Loss: 0.347357\n",
      "2025-05-10 12:12:17,569 - INFO - Epoch 10/100 | Train Loss: 0.340853 | Val Loss: 0.312025\n",
      "2025-05-10 12:12:18,008 - INFO - Epoch 11/100 | Train Loss: 0.320453 | Val Loss: 0.318732\n",
      "2025-05-10 12:12:18,483 - INFO - Epoch 12/100 | Train Loss: 0.304860 | Val Loss: 0.285874\n",
      "2025-05-10 12:12:18,959 - INFO - Epoch 13/100 | Train Loss: 0.289537 | Val Loss: 0.296334\n",
      "2025-05-10 12:12:19,441 - INFO - Epoch 14/100 | Train Loss: 0.277781 | Val Loss: 0.278292\n",
      "2025-05-10 12:12:19,893 - INFO - Epoch 15/100 | Train Loss: 0.268945 | Val Loss: 0.269551\n",
      "2025-05-10 12:12:20,373 - INFO - Epoch 16/100 | Train Loss: 0.257562 | Val Loss: 0.262166\n",
      "2025-05-10 12:12:20,853 - INFO - Epoch 17/100 | Train Loss: 0.250880 | Val Loss: 0.243944\n",
      "2025-05-10 12:12:21,286 - INFO - Epoch 18/100 | Train Loss: 0.250076 | Val Loss: 0.232266\n",
      "2025-05-10 12:12:21,738 - INFO - Epoch 19/100 | Train Loss: 0.238452 | Val Loss: 0.232452\n",
      "2025-05-10 12:12:22,162 - INFO - Epoch 20/100 | Train Loss: 0.230620 | Val Loss: 0.225186\n",
      "2025-05-10 12:12:22,627 - INFO - Epoch 21/100 | Train Loss: 0.230419 | Val Loss: 0.221836\n",
      "2025-05-10 12:12:23,100 - INFO - Epoch 22/100 | Train Loss: 0.222494 | Val Loss: 0.224866\n",
      "2025-05-10 12:12:23,580 - INFO - Epoch 23/100 | Train Loss: 0.218900 | Val Loss: 0.214321\n",
      "2025-05-10 12:12:24,029 - INFO - Epoch 24/100 | Train Loss: 0.209454 | Val Loss: 0.202490\n",
      "2025-05-10 12:12:24,500 - INFO - Epoch 25/100 | Train Loss: 0.204767 | Val Loss: 0.217014\n",
      "2025-05-10 12:12:24,944 - INFO - Epoch 26/100 | Train Loss: 0.209335 | Val Loss: 0.197850\n",
      "2025-05-10 12:12:25,384 - INFO - Epoch 27/100 | Train Loss: 0.194788 | Val Loss: 0.205258\n",
      "2025-05-10 12:12:25,831 - INFO - Epoch 28/100 | Train Loss: 0.190735 | Val Loss: 0.210873\n",
      "2025-05-10 12:12:26,290 - INFO - Epoch 29/100 | Train Loss: 0.197205 | Val Loss: 0.200476\n",
      "2025-05-10 12:12:26,764 - INFO - Epoch 30/100 | Train Loss: 0.201903 | Val Loss: 0.202380\n",
      "2025-05-10 12:12:27,213 - INFO - Epoch 31/100 | Train Loss: 0.203896 | Val Loss: 0.206145\n",
      "2025-05-10 12:12:27,703 - INFO - Epoch 32/100 | Train Loss: 0.196143 | Val Loss: 0.179200\n",
      "2025-05-10 12:12:28,218 - INFO - Epoch 33/100 | Train Loss: 0.191578 | Val Loss: 0.187327\n",
      "2025-05-10 12:12:28,699 - INFO - Epoch 34/100 | Train Loss: 0.184596 | Val Loss: 0.182576\n",
      "2025-05-10 12:12:29,163 - INFO - Epoch 35/100 | Train Loss: 0.184624 | Val Loss: 0.169599\n",
      "2025-05-10 12:12:29,609 - INFO - Epoch 36/100 | Train Loss: 0.177774 | Val Loss: 0.161301\n",
      "2025-05-10 12:12:30,043 - INFO - Epoch 37/100 | Train Loss: 0.189572 | Val Loss: 0.184022\n",
      "2025-05-10 12:12:30,479 - INFO - Epoch 38/100 | Train Loss: 0.187758 | Val Loss: 0.188661\n",
      "2025-05-10 12:12:30,940 - INFO - Epoch 39/100 | Train Loss: 0.176001 | Val Loss: 0.175701\n",
      "2025-05-10 12:12:31,364 - INFO - Epoch 40/100 | Train Loss: 0.182635 | Val Loss: 0.189245\n",
      "2025-05-10 12:12:31,788 - INFO - Epoch 41/100 | Train Loss: 0.185200 | Val Loss: 0.184584\n",
      "2025-05-10 12:12:32,257 - INFO - Epoch 42/100 | Train Loss: 0.180245 | Val Loss: 0.183104\n",
      "2025-05-10 12:12:32,735 - INFO - Epoch 43/100 | Train Loss: 0.178712 | Val Loss: 0.184028\n",
      "2025-05-10 12:12:33,261 - INFO - Epoch 44/100 | Train Loss: 0.176798 | Val Loss: 0.186665\n",
      "2025-05-10 12:12:33,702 - INFO - Epoch 45/100 | Train Loss: 0.179331 | Val Loss: 0.170139\n",
      "2025-05-10 12:12:34,131 - INFO - Epoch 46/100 | Train Loss: 0.184267 | Val Loss: 0.189932\n",
      "2025-05-10 12:12:34,132 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:12:34,132 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 16, 'learning_rate': 0.0005, 'window_size': 96}\n",
      "2025-05-10 12:12:34,651 - INFO - Epoch 1/100 | Train Loss: 0.988352 | Val Loss: 0.948709\n",
      "2025-05-10 12:12:35,103 - INFO - Epoch 2/100 | Train Loss: 0.887992 | Val Loss: 0.824567\n",
      "2025-05-10 12:12:35,549 - INFO - Epoch 3/100 | Train Loss: 0.786634 | Val Loss: 0.750710\n",
      "2025-05-10 12:12:36,035 - INFO - Epoch 4/100 | Train Loss: 0.710002 | Val Loss: 0.681834\n",
      "2025-05-10 12:12:36,585 - INFO - Epoch 5/100 | Train Loss: 0.661723 | Val Loss: 0.644289\n",
      "2025-05-10 12:12:37,026 - INFO - Epoch 6/100 | Train Loss: 0.621917 | Val Loss: 0.615012\n",
      "2025-05-10 12:12:37,455 - INFO - Epoch 7/100 | Train Loss: 0.596273 | Val Loss: 0.579472\n",
      "2025-05-10 12:12:37,931 - INFO - Epoch 8/100 | Train Loss: 0.574036 | Val Loss: 0.569316\n",
      "2025-05-10 12:12:38,394 - INFO - Epoch 9/100 | Train Loss: 0.555797 | Val Loss: 0.545534\n",
      "2025-05-10 12:12:38,855 - INFO - Epoch 10/100 | Train Loss: 0.541450 | Val Loss: 0.535304\n",
      "2025-05-10 12:12:39,315 - INFO - Epoch 11/100 | Train Loss: 0.526297 | Val Loss: 0.518215\n",
      "2025-05-10 12:12:39,807 - INFO - Epoch 12/100 | Train Loss: 0.515362 | Val Loss: 0.512229\n",
      "2025-05-10 12:12:40,290 - INFO - Epoch 13/100 | Train Loss: 0.504936 | Val Loss: 0.503290\n",
      "2025-05-10 12:12:40,769 - INFO - Epoch 14/100 | Train Loss: 0.499393 | Val Loss: 0.500294\n",
      "2025-05-10 12:12:41,245 - INFO - Epoch 15/100 | Train Loss: 0.487744 | Val Loss: 0.492381\n",
      "2025-05-10 12:12:41,694 - INFO - Epoch 16/100 | Train Loss: 0.482482 | Val Loss: 0.475980\n",
      "2025-05-10 12:12:42,180 - INFO - Epoch 17/100 | Train Loss: 0.483265 | Val Loss: 0.482099\n",
      "2025-05-10 12:12:42,641 - INFO - Epoch 18/100 | Train Loss: 0.471425 | Val Loss: 0.479836\n",
      "2025-05-10 12:12:43,129 - INFO - Epoch 19/100 | Train Loss: 0.476142 | Val Loss: 0.466430\n",
      "2025-05-10 12:12:43,610 - INFO - Epoch 20/100 | Train Loss: 0.472409 | Val Loss: 0.461964\n",
      "2025-05-10 12:12:44,082 - INFO - Epoch 21/100 | Train Loss: 0.463518 | Val Loss: 0.458472\n",
      "2025-05-10 12:12:44,555 - INFO - Epoch 22/100 | Train Loss: 0.464264 | Val Loss: 0.463381\n",
      "2025-05-10 12:12:45,067 - INFO - Epoch 23/100 | Train Loss: 0.458436 | Val Loss: 0.453668\n",
      "2025-05-10 12:12:45,520 - INFO - Epoch 24/100 | Train Loss: 0.460317 | Val Loss: 0.471148\n",
      "2025-05-10 12:12:45,969 - INFO - Epoch 25/100 | Train Loss: 0.461406 | Val Loss: 0.460191\n",
      "2025-05-10 12:12:46,464 - INFO - Epoch 26/100 | Train Loss: 0.455669 | Val Loss: 0.445788\n",
      "2025-05-10 12:12:46,942 - INFO - Epoch 27/100 | Train Loss: 0.458013 | Val Loss: 0.460265\n",
      "2025-05-10 12:12:47,421 - INFO - Epoch 28/100 | Train Loss: 0.457915 | Val Loss: 0.445876\n",
      "2025-05-10 12:12:47,867 - INFO - Epoch 29/100 | Train Loss: 0.457486 | Val Loss: 0.445866\n",
      "2025-05-10 12:12:48,383 - INFO - Epoch 30/100 | Train Loss: 0.454380 | Val Loss: 0.443925\n",
      "2025-05-10 12:12:48,917 - INFO - Epoch 31/100 | Train Loss: 0.460064 | Val Loss: 0.459054\n",
      "2025-05-10 12:12:49,473 - INFO - Epoch 32/100 | Train Loss: 0.452145 | Val Loss: 0.461908\n",
      "2025-05-10 12:12:49,929 - INFO - Epoch 33/100 | Train Loss: 0.450124 | Val Loss: 0.456044\n",
      "2025-05-10 12:12:50,383 - INFO - Epoch 34/100 | Train Loss: 0.455527 | Val Loss: 0.456887\n",
      "2025-05-10 12:12:50,819 - INFO - Epoch 35/100 | Train Loss: 0.446501 | Val Loss: 0.447032\n",
      "2025-05-10 12:12:51,284 - INFO - Epoch 36/100 | Train Loss: 0.457206 | Val Loss: 0.451077\n",
      "2025-05-10 12:12:51,776 - INFO - Epoch 37/100 | Train Loss: 0.450664 | Val Loss: 0.459771\n",
      "2025-05-10 12:12:52,208 - INFO - Epoch 38/100 | Train Loss: 0.454021 | Val Loss: 0.457933\n",
      "2025-05-10 12:12:52,659 - INFO - Epoch 39/100 | Train Loss: 0.449630 | Val Loss: 0.463608\n",
      "2025-05-10 12:12:53,136 - INFO - Epoch 40/100 | Train Loss: 0.453067 | Val Loss: 0.453318\n",
      "2025-05-10 12:12:53,137 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:12:53,137 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 32, 'learning_rate': 0.001, 'window_size': 64}\n",
      "2025-05-10 12:12:53,576 - INFO - Epoch 1/100 | Train Loss: 0.916432 | Val Loss: 0.760478\n",
      "2025-05-10 12:12:54,020 - INFO - Epoch 2/100 | Train Loss: 0.644936 | Val Loss: 0.565678\n",
      "2025-05-10 12:12:54,449 - INFO - Epoch 3/100 | Train Loss: 0.512747 | Val Loss: 0.466147\n",
      "2025-05-10 12:12:54,921 - INFO - Epoch 4/100 | Train Loss: 0.436858 | Val Loss: 0.407496\n",
      "2025-05-10 12:12:55,365 - INFO - Epoch 5/100 | Train Loss: 0.376486 | Val Loss: 0.367450\n",
      "2025-05-10 12:12:55,802 - INFO - Epoch 6/100 | Train Loss: 0.346790 | Val Loss: 0.348692\n",
      "2025-05-10 12:12:56,235 - INFO - Epoch 7/100 | Train Loss: 0.315414 | Val Loss: 0.302944\n",
      "2025-05-10 12:12:56,697 - INFO - Epoch 8/100 | Train Loss: 0.307646 | Val Loss: 0.292062\n",
      "2025-05-10 12:12:57,161 - INFO - Epoch 9/100 | Train Loss: 0.296440 | Val Loss: 0.277132\n",
      "2025-05-10 12:12:57,623 - INFO - Epoch 10/100 | Train Loss: 0.274957 | Val Loss: 0.276776\n",
      "2025-05-10 12:12:58,041 - INFO - Epoch 11/100 | Train Loss: 0.263247 | Val Loss: 0.257783\n",
      "2025-05-10 12:12:58,474 - INFO - Epoch 12/100 | Train Loss: 0.255062 | Val Loss: 0.233708\n",
      "2025-05-10 12:12:58,935 - INFO - Epoch 13/100 | Train Loss: 0.242807 | Val Loss: 0.231802\n",
      "2025-05-10 12:12:59,363 - INFO - Epoch 14/100 | Train Loss: 0.231083 | Val Loss: 0.225652\n",
      "2025-05-10 12:12:59,809 - INFO - Epoch 15/100 | Train Loss: 0.233924 | Val Loss: 0.255467\n",
      "2025-05-10 12:13:00,236 - INFO - Epoch 16/100 | Train Loss: 0.228849 | Val Loss: 0.214986\n",
      "2025-05-10 12:13:00,664 - INFO - Epoch 17/100 | Train Loss: 0.224379 | Val Loss: 0.222371\n",
      "2025-05-10 12:13:01,113 - INFO - Epoch 18/100 | Train Loss: 0.216400 | Val Loss: 0.202854\n",
      "2025-05-10 12:13:01,508 - INFO - Epoch 19/100 | Train Loss: 0.206402 | Val Loss: 0.208684\n",
      "2025-05-10 12:13:01,961 - INFO - Epoch 20/100 | Train Loss: 0.205192 | Val Loss: 0.220333\n",
      "2025-05-10 12:13:02,394 - INFO - Epoch 21/100 | Train Loss: 0.214169 | Val Loss: 0.206173\n",
      "2025-05-10 12:13:02,800 - INFO - Epoch 22/100 | Train Loss: 0.204305 | Val Loss: 0.210137\n",
      "2025-05-10 12:13:03,238 - INFO - Epoch 23/100 | Train Loss: 0.208717 | Val Loss: 0.196171\n",
      "2025-05-10 12:13:03,629 - INFO - Epoch 24/100 | Train Loss: 0.206162 | Val Loss: 0.197133\n",
      "2025-05-10 12:13:04,049 - INFO - Epoch 25/100 | Train Loss: 0.204315 | Val Loss: 0.215952\n",
      "2025-05-10 12:13:04,500 - INFO - Epoch 26/100 | Train Loss: 0.204158 | Val Loss: 0.208648\n",
      "2025-05-10 12:13:04,980 - INFO - Epoch 27/100 | Train Loss: 0.194163 | Val Loss: 0.207005\n",
      "2025-05-10 12:13:05,428 - INFO - Epoch 28/100 | Train Loss: 0.200376 | Val Loss: 0.195798\n",
      "2025-05-10 12:13:05,826 - INFO - Epoch 29/100 | Train Loss: 0.195657 | Val Loss: 0.187758\n",
      "2025-05-10 12:13:06,235 - INFO - Epoch 30/100 | Train Loss: 0.207858 | Val Loss: 0.222725\n",
      "2025-05-10 12:13:06,646 - INFO - Epoch 31/100 | Train Loss: 0.202672 | Val Loss: 0.198532\n",
      "2025-05-10 12:13:07,083 - INFO - Epoch 32/100 | Train Loss: 0.189845 | Val Loss: 0.203720\n",
      "2025-05-10 12:13:07,519 - INFO - Epoch 33/100 | Train Loss: 0.193565 | Val Loss: 0.191380\n",
      "2025-05-10 12:13:07,922 - INFO - Epoch 34/100 | Train Loss: 0.208875 | Val Loss: 0.199895\n",
      "2025-05-10 12:13:08,344 - INFO - Epoch 35/100 | Train Loss: 0.200009 | Val Loss: 0.188953\n",
      "2025-05-10 12:13:08,754 - INFO - Epoch 36/100 | Train Loss: 0.198837 | Val Loss: 0.190508\n",
      "2025-05-10 12:13:09,222 - INFO - Epoch 37/100 | Train Loss: 0.193480 | Val Loss: 0.201583\n",
      "2025-05-10 12:13:09,630 - INFO - Epoch 38/100 | Train Loss: 0.196169 | Val Loss: 0.186628\n",
      "2025-05-10 12:13:10,032 - INFO - Epoch 39/100 | Train Loss: 0.202569 | Val Loss: 0.211331\n",
      "2025-05-10 12:13:10,441 - INFO - Epoch 40/100 | Train Loss: 0.203477 | Val Loss: 0.193350\n",
      "2025-05-10 12:13:10,864 - INFO - Epoch 41/100 | Train Loss: 0.189843 | Val Loss: 0.176888\n",
      "2025-05-10 12:13:11,278 - INFO - Epoch 42/100 | Train Loss: 0.185509 | Val Loss: 0.187136\n",
      "2025-05-10 12:13:11,665 - INFO - Epoch 43/100 | Train Loss: 0.194341 | Val Loss: 0.208069\n",
      "2025-05-10 12:13:12,075 - INFO - Epoch 44/100 | Train Loss: 0.190312 | Val Loss: 0.181394\n",
      "2025-05-10 12:13:12,478 - INFO - Epoch 45/100 | Train Loss: 0.194072 | Val Loss: 0.188752\n",
      "2025-05-10 12:13:12,894 - INFO - Epoch 46/100 | Train Loss: 0.190727 | Val Loss: 0.192471\n",
      "2025-05-10 12:13:13,304 - INFO - Epoch 47/100 | Train Loss: 0.196808 | Val Loss: 0.193629\n",
      "2025-05-10 12:13:13,742 - INFO - Epoch 48/100 | Train Loss: 0.190683 | Val Loss: 0.192682\n",
      "2025-05-10 12:13:14,161 - INFO - Epoch 49/100 | Train Loss: 0.196811 | Val Loss: 0.198317\n",
      "2025-05-10 12:13:14,570 - INFO - Epoch 50/100 | Train Loss: 0.191850 | Val Loss: 0.197341\n",
      "2025-05-10 12:13:15,041 - INFO - Epoch 51/100 | Train Loss: 0.185828 | Val Loss: 0.191711\n",
      "2025-05-10 12:13:15,042 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:13:15,043 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 32, 'learning_rate': 0.001, 'window_size': 96}\n",
      "2025-05-10 12:13:15,447 - INFO - Epoch 1/100 | Train Loss: 0.950230 | Val Loss: 0.858129\n",
      "2025-05-10 12:13:15,888 - INFO - Epoch 2/100 | Train Loss: 0.773042 | Val Loss: 0.722490\n",
      "2025-05-10 12:13:16,305 - INFO - Epoch 3/100 | Train Loss: 0.668800 | Val Loss: 0.630162\n",
      "2025-05-10 12:13:16,716 - INFO - Epoch 4/100 | Train Loss: 0.608299 | Val Loss: 0.597891\n",
      "2025-05-10 12:13:17,146 - INFO - Epoch 5/100 | Train Loss: 0.577610 | Val Loss: 0.572674\n",
      "2025-05-10 12:13:17,567 - INFO - Epoch 6/100 | Train Loss: 0.552376 | Val Loss: 0.546373\n",
      "2025-05-10 12:13:18,000 - INFO - Epoch 7/100 | Train Loss: 0.535879 | Val Loss: 0.536226\n",
      "2025-05-10 12:13:18,420 - INFO - Epoch 8/100 | Train Loss: 0.524110 | Val Loss: 0.510017\n",
      "2025-05-10 12:13:18,845 - INFO - Epoch 9/100 | Train Loss: 0.506158 | Val Loss: 0.514202\n",
      "2025-05-10 12:13:19,299 - INFO - Epoch 10/100 | Train Loss: 0.503656 | Val Loss: 0.509024\n",
      "2025-05-10 12:13:19,709 - INFO - Epoch 11/100 | Train Loss: 0.497350 | Val Loss: 0.498959\n",
      "2025-05-10 12:13:20,143 - INFO - Epoch 12/100 | Train Loss: 0.495601 | Val Loss: 0.500480\n",
      "2025-05-10 12:13:20,605 - INFO - Epoch 13/100 | Train Loss: 0.491434 | Val Loss: 0.490151\n",
      "2025-05-10 12:13:21,141 - INFO - Epoch 14/100 | Train Loss: 0.490772 | Val Loss: 0.487776\n",
      "2025-05-10 12:13:21,619 - INFO - Epoch 15/100 | Train Loss: 0.498689 | Val Loss: 0.484990\n",
      "2025-05-10 12:13:22,068 - INFO - Epoch 16/100 | Train Loss: 0.490638 | Val Loss: 0.488149\n",
      "2025-05-10 12:13:22,558 - INFO - Epoch 17/100 | Train Loss: 0.486239 | Val Loss: 0.490835\n",
      "2025-05-10 12:13:22,974 - INFO - Epoch 18/100 | Train Loss: 0.483923 | Val Loss: 0.491518\n",
      "2025-05-10 12:13:23,419 - INFO - Epoch 19/100 | Train Loss: 0.491648 | Val Loss: 0.499327\n",
      "2025-05-10 12:13:23,846 - INFO - Epoch 20/100 | Train Loss: 0.488217 | Val Loss: 0.500663\n",
      "2025-05-10 12:13:24,260 - INFO - Epoch 21/100 | Train Loss: 0.489995 | Val Loss: 0.495794\n",
      "2025-05-10 12:13:24,721 - INFO - Epoch 22/100 | Train Loss: 0.487366 | Val Loss: 0.493416\n",
      "2025-05-10 12:13:25,182 - INFO - Epoch 23/100 | Train Loss: 0.493567 | Val Loss: 0.497095\n",
      "2025-05-10 12:13:25,659 - INFO - Epoch 24/100 | Train Loss: 0.484076 | Val Loss: 0.476143\n",
      "2025-05-10 12:13:26,129 - INFO - Epoch 25/100 | Train Loss: 0.492504 | Val Loss: 0.484531\n",
      "2025-05-10 12:13:26,625 - INFO - Epoch 26/100 | Train Loss: 0.486384 | Val Loss: 0.490797\n",
      "2025-05-10 12:13:27,060 - INFO - Epoch 27/100 | Train Loss: 0.487843 | Val Loss: 0.493564\n",
      "2025-05-10 12:13:27,526 - INFO - Epoch 28/100 | Train Loss: 0.488790 | Val Loss: 0.495503\n",
      "2025-05-10 12:13:28,016 - INFO - Epoch 29/100 | Train Loss: 0.490044 | Val Loss: 0.484024\n",
      "2025-05-10 12:13:28,481 - INFO - Epoch 30/100 | Train Loss: 0.490579 | Val Loss: 0.486465\n",
      "2025-05-10 12:13:29,012 - INFO - Epoch 31/100 | Train Loss: 0.488054 | Val Loss: 0.485394\n",
      "2025-05-10 12:13:29,555 - INFO - Epoch 32/100 | Train Loss: 0.485299 | Val Loss: 0.483414\n",
      "2025-05-10 12:13:30,052 - INFO - Epoch 33/100 | Train Loss: 0.487505 | Val Loss: 0.493436\n",
      "2025-05-10 12:13:30,526 - INFO - Epoch 34/100 | Train Loss: 0.486250 | Val Loss: 0.490502\n",
      "2025-05-10 12:13:30,527 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:13:30,528 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 32, 'learning_rate': 0.0005, 'window_size': 64}\n",
      "2025-05-10 12:13:31,005 - INFO - Epoch 1/100 | Train Loss: 0.970291 | Val Loss: 0.921849\n",
      "2025-05-10 12:13:31,471 - INFO - Epoch 2/100 | Train Loss: 0.820147 | Val Loss: 0.726761\n",
      "2025-05-10 12:13:31,957 - INFO - Epoch 3/100 | Train Loss: 0.663860 | Val Loss: 0.608063\n",
      "2025-05-10 12:13:32,450 - INFO - Epoch 4/100 | Train Loss: 0.569839 | Val Loss: 0.520076\n",
      "2025-05-10 12:13:32,931 - INFO - Epoch 5/100 | Train Loss: 0.507102 | Val Loss: 0.479892\n",
      "2025-05-10 12:13:33,432 - INFO - Epoch 6/100 | Train Loss: 0.456074 | Val Loss: 0.452406\n",
      "2025-05-10 12:13:33,936 - INFO - Epoch 7/100 | Train Loss: 0.421411 | Val Loss: 0.399857\n",
      "2025-05-10 12:13:34,443 - INFO - Epoch 8/100 | Train Loss: 0.388015 | Val Loss: 0.383732\n",
      "2025-05-10 12:13:34,925 - INFO - Epoch 9/100 | Train Loss: 0.377772 | Val Loss: 0.358698\n",
      "2025-05-10 12:13:35,403 - INFO - Epoch 10/100 | Train Loss: 0.353336 | Val Loss: 0.329062\n",
      "2025-05-10 12:13:35,863 - INFO - Epoch 11/100 | Train Loss: 0.334431 | Val Loss: 0.320543\n",
      "2025-05-10 12:13:36,434 - INFO - Epoch 12/100 | Train Loss: 0.324151 | Val Loss: 0.322724\n",
      "2025-05-10 12:13:37,035 - INFO - Epoch 13/100 | Train Loss: 0.317286 | Val Loss: 0.293384\n",
      "2025-05-10 12:13:37,571 - INFO - Epoch 14/100 | Train Loss: 0.302587 | Val Loss: 0.290050\n",
      "2025-05-10 12:13:38,088 - INFO - Epoch 15/100 | Train Loss: 0.292902 | Val Loss: 0.296752\n",
      "2025-05-10 12:13:38,600 - INFO - Epoch 16/100 | Train Loss: 0.278018 | Val Loss: 0.274829\n",
      "2025-05-10 12:13:39,072 - INFO - Epoch 17/100 | Train Loss: 0.277036 | Val Loss: 0.282197\n",
      "2025-05-10 12:13:39,551 - INFO - Epoch 18/100 | Train Loss: 0.268139 | Val Loss: 0.266722\n",
      "2025-05-10 12:13:40,030 - INFO - Epoch 19/100 | Train Loss: 0.260034 | Val Loss: 0.268087\n",
      "2025-05-10 12:13:40,510 - INFO - Epoch 20/100 | Train Loss: 0.253613 | Val Loss: 0.254870\n",
      "2025-05-10 12:13:40,977 - INFO - Epoch 21/100 | Train Loss: 0.256729 | Val Loss: 0.262761\n",
      "2025-05-10 12:13:41,480 - INFO - Epoch 22/100 | Train Loss: 0.256927 | Val Loss: 0.250559\n",
      "2025-05-10 12:13:41,950 - INFO - Epoch 23/100 | Train Loss: 0.244284 | Val Loss: 0.241666\n",
      "2025-05-10 12:13:42,474 - INFO - Epoch 24/100 | Train Loss: 0.244248 | Val Loss: 0.241091\n",
      "2025-05-10 12:13:42,973 - INFO - Epoch 25/100 | Train Loss: 0.247619 | Val Loss: 0.252381\n",
      "2025-05-10 12:13:43,480 - INFO - Epoch 26/100 | Train Loss: 0.239269 | Val Loss: 0.245563\n",
      "2025-05-10 12:13:43,943 - INFO - Epoch 27/100 | Train Loss: 0.240174 | Val Loss: 0.245013\n",
      "2025-05-10 12:13:44,456 - INFO - Epoch 28/100 | Train Loss: 0.236615 | Val Loss: 0.245521\n",
      "2025-05-10 12:13:44,934 - INFO - Epoch 29/100 | Train Loss: 0.236491 | Val Loss: 0.242813\n",
      "2025-05-10 12:13:45,469 - INFO - Epoch 30/100 | Train Loss: 0.233550 | Val Loss: 0.213535\n",
      "2025-05-10 12:13:46,011 - INFO - Epoch 31/100 | Train Loss: 0.226059 | Val Loss: 0.214228\n",
      "2025-05-10 12:13:46,494 - INFO - Epoch 32/100 | Train Loss: 0.214937 | Val Loss: 0.214523\n",
      "2025-05-10 12:13:47,004 - INFO - Epoch 33/100 | Train Loss: 0.219571 | Val Loss: 0.214197\n",
      "2025-05-10 12:13:47,537 - INFO - Epoch 34/100 | Train Loss: 0.215317 | Val Loss: 0.212579\n",
      "2025-05-10 12:13:48,029 - INFO - Epoch 35/100 | Train Loss: 0.210688 | Val Loss: 0.224199\n",
      "2025-05-10 12:13:48,536 - INFO - Epoch 36/100 | Train Loss: 0.212697 | Val Loss: 0.208250\n",
      "2025-05-10 12:13:49,037 - INFO - Epoch 37/100 | Train Loss: 0.211433 | Val Loss: 0.211921\n",
      "2025-05-10 12:13:49,535 - INFO - Epoch 38/100 | Train Loss: 0.204472 | Val Loss: 0.214232\n",
      "2025-05-10 12:13:50,080 - INFO - Epoch 39/100 | Train Loss: 0.212232 | Val Loss: 0.204854\n",
      "2025-05-10 12:13:50,571 - INFO - Epoch 40/100 | Train Loss: 0.198174 | Val Loss: 0.213557\n",
      "2025-05-10 12:13:51,076 - INFO - Epoch 41/100 | Train Loss: 0.204592 | Val Loss: 0.193910\n",
      "2025-05-10 12:13:51,591 - INFO - Epoch 42/100 | Train Loss: 0.199018 | Val Loss: 0.205235\n",
      "2025-05-10 12:13:52,199 - INFO - Epoch 43/100 | Train Loss: 0.210380 | Val Loss: 0.210405\n",
      "2025-05-10 12:13:52,734 - INFO - Epoch 44/100 | Train Loss: 0.205022 | Val Loss: 0.208125\n",
      "2025-05-10 12:13:53,365 - INFO - Epoch 45/100 | Train Loss: 0.206165 | Val Loss: 0.208576\n",
      "2025-05-10 12:13:53,864 - INFO - Epoch 46/100 | Train Loss: 0.196172 | Val Loss: 0.198603\n",
      "2025-05-10 12:13:54,398 - INFO - Epoch 47/100 | Train Loss: 0.211269 | Val Loss: 0.200325\n",
      "2025-05-10 12:13:54,880 - INFO - Epoch 48/100 | Train Loss: 0.201449 | Val Loss: 0.212511\n",
      "2025-05-10 12:13:55,365 - INFO - Epoch 49/100 | Train Loss: 0.195792 | Val Loss: 0.181952\n",
      "2025-05-10 12:13:55,842 - INFO - Epoch 50/100 | Train Loss: 0.189733 | Val Loss: 0.194053\n",
      "2025-05-10 12:13:56,360 - INFO - Epoch 51/100 | Train Loss: 0.201299 | Val Loss: 0.213879\n",
      "2025-05-10 12:13:56,881 - INFO - Epoch 52/100 | Train Loss: 0.200043 | Val Loss: 0.204012\n",
      "2025-05-10 12:13:57,394 - INFO - Epoch 53/100 | Train Loss: 0.194567 | Val Loss: 0.195588\n",
      "2025-05-10 12:13:57,867 - INFO - Epoch 54/100 | Train Loss: 0.195359 | Val Loss: 0.188945\n",
      "2025-05-10 12:13:58,368 - INFO - Epoch 55/100 | Train Loss: 0.195769 | Val Loss: 0.190036\n",
      "2025-05-10 12:13:58,832 - INFO - Epoch 56/100 | Train Loss: 0.200622 | Val Loss: 0.199625\n",
      "2025-05-10 12:13:59,293 - INFO - Epoch 57/100 | Train Loss: 0.190463 | Val Loss: 0.192973\n",
      "2025-05-10 12:13:59,784 - INFO - Epoch 58/100 | Train Loss: 0.195229 | Val Loss: 0.215420\n",
      "2025-05-10 12:14:00,317 - INFO - Epoch 59/100 | Train Loss: 0.201921 | Val Loss: 0.197291\n",
      "2025-05-10 12:14:00,318 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:14:00,319 - INFO - Training with params: {'batch_size': 16, 'latent_dim': 32, 'learning_rate': 0.0005, 'window_size': 96}\n",
      "2025-05-10 12:14:00,797 - INFO - Epoch 1/100 | Train Loss: 0.985977 | Val Loss: 0.943715\n",
      "2025-05-10 12:14:01,334 - INFO - Epoch 2/100 | Train Loss: 0.877636 | Val Loss: 0.814900\n",
      "2025-05-10 12:14:01,803 - INFO - Epoch 3/100 | Train Loss: 0.768988 | Val Loss: 0.723814\n",
      "2025-05-10 12:14:02,334 - INFO - Epoch 4/100 | Train Loss: 0.688089 | Val Loss: 0.658947\n",
      "2025-05-10 12:14:02,860 - INFO - Epoch 5/100 | Train Loss: 0.640206 | Val Loss: 0.608340\n",
      "2025-05-10 12:14:03,355 - INFO - Epoch 6/100 | Train Loss: 0.604165 | Val Loss: 0.592615\n",
      "2025-05-10 12:14:03,840 - INFO - Epoch 7/100 | Train Loss: 0.578694 | Val Loss: 0.558157\n",
      "2025-05-10 12:14:04,307 - INFO - Epoch 8/100 | Train Loss: 0.557707 | Val Loss: 0.553234\n",
      "2025-05-10 12:14:04,933 - INFO - Epoch 9/100 | Train Loss: 0.541432 | Val Loss: 0.543082\n",
      "2025-05-10 12:14:05,432 - INFO - Epoch 10/100 | Train Loss: 0.526549 | Val Loss: 0.522335\n",
      "2025-05-10 12:14:05,910 - INFO - Epoch 11/100 | Train Loss: 0.515485 | Val Loss: 0.515332\n",
      "2025-05-10 12:14:06,409 - INFO - Epoch 12/100 | Train Loss: 0.498956 | Val Loss: 0.494804\n",
      "2025-05-10 12:14:06,911 - INFO - Epoch 13/100 | Train Loss: 0.496164 | Val Loss: 0.487973\n",
      "2025-05-10 12:14:07,375 - INFO - Epoch 14/100 | Train Loss: 0.490088 | Val Loss: 0.486694\n",
      "2025-05-10 12:14:07,869 - INFO - Epoch 15/100 | Train Loss: 0.479245 | Val Loss: 0.478501\n",
      "2025-05-10 12:14:08,461 - INFO - Epoch 16/100 | Train Loss: 0.471102 | Val Loss: 0.467968\n",
      "2025-05-10 12:14:09,037 - INFO - Epoch 17/100 | Train Loss: 0.465363 | Val Loss: 0.461560\n",
      "2025-05-10 12:14:09,661 - INFO - Epoch 18/100 | Train Loss: 0.471755 | Val Loss: 0.466316\n",
      "2025-05-10 12:14:10,187 - INFO - Epoch 19/100 | Train Loss: 0.464891 | Val Loss: 0.463318\n",
      "2025-05-10 12:14:10,655 - INFO - Epoch 20/100 | Train Loss: 0.453671 | Val Loss: 0.451937\n",
      "2025-05-10 12:14:11,152 - INFO - Epoch 21/100 | Train Loss: 0.458689 | Val Loss: 0.449143\n",
      "2025-05-10 12:14:11,593 - INFO - Epoch 22/100 | Train Loss: 0.450629 | Val Loss: 0.455919\n",
      "2025-05-10 12:14:12,065 - INFO - Epoch 23/100 | Train Loss: 0.449215 | Val Loss: 0.450500\n",
      "2025-05-10 12:14:12,549 - INFO - Epoch 24/100 | Train Loss: 0.450486 | Val Loss: 0.429892\n",
      "2025-05-10 12:14:13,010 - INFO - Epoch 25/100 | Train Loss: 0.449425 | Val Loss: 0.456053\n",
      "2025-05-10 12:14:13,476 - INFO - Epoch 26/100 | Train Loss: 0.443754 | Val Loss: 0.448669\n",
      "2025-05-10 12:14:13,956 - INFO - Epoch 27/100 | Train Loss: 0.445190 | Val Loss: 0.448852\n",
      "2025-05-10 12:14:14,472 - INFO - Epoch 28/100 | Train Loss: 0.448162 | Val Loss: 0.445864\n",
      "2025-05-10 12:14:14,948 - INFO - Epoch 29/100 | Train Loss: 0.445609 | Val Loss: 0.435276\n",
      "2025-05-10 12:14:15,448 - INFO - Epoch 30/100 | Train Loss: 0.446517 | Val Loss: 0.441035\n",
      "2025-05-10 12:14:15,918 - INFO - Epoch 31/100 | Train Loss: 0.446111 | Val Loss: 0.443332\n",
      "2025-05-10 12:14:16,408 - INFO - Epoch 32/100 | Train Loss: 0.442580 | Val Loss: 0.451865\n",
      "2025-05-10 12:14:16,884 - INFO - Epoch 33/100 | Train Loss: 0.441730 | Val Loss: 0.424992\n",
      "2025-05-10 12:14:17,405 - INFO - Epoch 34/100 | Train Loss: 0.440950 | Val Loss: 0.448981\n",
      "2025-05-10 12:14:17,877 - INFO - Epoch 35/100 | Train Loss: 0.442865 | Val Loss: 0.444296\n",
      "2025-05-10 12:14:18,395 - INFO - Epoch 36/100 | Train Loss: 0.438927 | Val Loss: 0.444166\n",
      "2025-05-10 12:14:18,898 - INFO - Epoch 37/100 | Train Loss: 0.441640 | Val Loss: 0.436151\n",
      "2025-05-10 12:14:19,402 - INFO - Epoch 38/100 | Train Loss: 0.437439 | Val Loss: 0.441451\n",
      "2025-05-10 12:14:19,968 - INFO - Epoch 39/100 | Train Loss: 0.443732 | Val Loss: 0.436134\n",
      "2025-05-10 12:14:20,498 - INFO - Epoch 40/100 | Train Loss: 0.430712 | Val Loss: 0.433217\n",
      "2025-05-10 12:14:21,001 - INFO - Epoch 41/100 | Train Loss: 0.437704 | Val Loss: 0.434182\n",
      "2025-05-10 12:14:21,489 - INFO - Epoch 42/100 | Train Loss: 0.433698 | Val Loss: 0.433263\n",
      "2025-05-10 12:14:22,000 - INFO - Epoch 43/100 | Train Loss: 0.433355 | Val Loss: 0.439745\n",
      "2025-05-10 12:14:22,001 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:14:22,003 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 16, 'learning_rate': 0.001, 'window_size': 64}\n",
      "2025-05-10 12:14:22,289 - INFO - Epoch 1/100 | Train Loss: 0.954995 | Val Loss: 0.863919\n",
      "2025-05-10 12:14:22,570 - INFO - Epoch 2/100 | Train Loss: 0.761890 | Val Loss: 0.674910\n",
      "2025-05-10 12:14:22,865 - INFO - Epoch 3/100 | Train Loss: 0.615348 | Val Loss: 0.565194\n",
      "2025-05-10 12:14:23,104 - INFO - Epoch 4/100 | Train Loss: 0.528296 | Val Loss: 0.479930\n",
      "2025-05-10 12:14:23,359 - INFO - Epoch 5/100 | Train Loss: 0.465889 | Val Loss: 0.437435\n",
      "2025-05-10 12:14:23,588 - INFO - Epoch 6/100 | Train Loss: 0.420321 | Val Loss: 0.397142\n",
      "2025-05-10 12:14:23,855 - INFO - Epoch 7/100 | Train Loss: 0.384770 | Val Loss: 0.365018\n",
      "2025-05-10 12:14:24,167 - INFO - Epoch 8/100 | Train Loss: 0.360829 | Val Loss: 0.358096\n",
      "2025-05-10 12:14:24,489 - INFO - Epoch 9/100 | Train Loss: 0.336231 | Val Loss: 0.325718\n",
      "2025-05-10 12:14:24,856 - INFO - Epoch 10/100 | Train Loss: 0.325404 | Val Loss: 0.316709\n",
      "2025-05-10 12:14:25,155 - INFO - Epoch 11/100 | Train Loss: 0.304154 | Val Loss: 0.277324\n",
      "2025-05-10 12:14:25,470 - INFO - Epoch 12/100 | Train Loss: 0.287296 | Val Loss: 0.289773\n",
      "2025-05-10 12:14:25,735 - INFO - Epoch 13/100 | Train Loss: 0.278819 | Val Loss: 0.263216\n",
      "2025-05-10 12:14:26,009 - INFO - Epoch 14/100 | Train Loss: 0.268938 | Val Loss: 0.259384\n",
      "2025-05-10 12:14:26,283 - INFO - Epoch 15/100 | Train Loss: 0.273535 | Val Loss: 0.254865\n",
      "2025-05-10 12:14:26,585 - INFO - Epoch 16/100 | Train Loss: 0.250206 | Val Loss: 0.247467\n",
      "2025-05-10 12:14:26,861 - INFO - Epoch 17/100 | Train Loss: 0.240435 | Val Loss: 0.240351\n",
      "2025-05-10 12:14:27,158 - INFO - Epoch 18/100 | Train Loss: 0.237055 | Val Loss: 0.249777\n",
      "2025-05-10 12:14:27,441 - INFO - Epoch 19/100 | Train Loss: 0.232953 | Val Loss: 0.222250\n",
      "2025-05-10 12:14:27,758 - INFO - Epoch 20/100 | Train Loss: 0.217840 | Val Loss: 0.207776\n",
      "2025-05-10 12:14:28,070 - INFO - Epoch 21/100 | Train Loss: 0.218600 | Val Loss: 0.212367\n",
      "2025-05-10 12:14:28,339 - INFO - Epoch 22/100 | Train Loss: 0.222361 | Val Loss: 0.203871\n",
      "2025-05-10 12:14:28,623 - INFO - Epoch 23/100 | Train Loss: 0.207460 | Val Loss: 0.220466\n",
      "2025-05-10 12:14:28,889 - INFO - Epoch 24/100 | Train Loss: 0.214806 | Val Loss: 0.214015\n",
      "2025-05-10 12:14:29,197 - INFO - Epoch 25/100 | Train Loss: 0.217237 | Val Loss: 0.199019\n",
      "2025-05-10 12:14:29,469 - INFO - Epoch 26/100 | Train Loss: 0.210259 | Val Loss: 0.211914\n",
      "2025-05-10 12:14:29,730 - INFO - Epoch 27/100 | Train Loss: 0.205068 | Val Loss: 0.201025\n",
      "2025-05-10 12:14:30,010 - INFO - Epoch 28/100 | Train Loss: 0.205045 | Val Loss: 0.200749\n",
      "2025-05-10 12:14:30,297 - INFO - Epoch 29/100 | Train Loss: 0.193805 | Val Loss: 0.193599\n",
      "2025-05-10 12:14:30,568 - INFO - Epoch 30/100 | Train Loss: 0.202269 | Val Loss: 0.196553\n",
      "2025-05-10 12:14:30,836 - INFO - Epoch 31/100 | Train Loss: 0.196153 | Val Loss: 0.190699\n",
      "2025-05-10 12:14:31,127 - INFO - Epoch 32/100 | Train Loss: 0.195226 | Val Loss: 0.197302\n",
      "2025-05-10 12:14:31,387 - INFO - Epoch 33/100 | Train Loss: 0.196307 | Val Loss: 0.177530\n",
      "2025-05-10 12:14:31,670 - INFO - Epoch 34/100 | Train Loss: 0.186254 | Val Loss: 0.175746\n",
      "2025-05-10 12:14:31,951 - INFO - Epoch 35/100 | Train Loss: 0.186451 | Val Loss: 0.184241\n",
      "2025-05-10 12:14:32,231 - INFO - Epoch 36/100 | Train Loss: 0.189367 | Val Loss: 0.182199\n",
      "2025-05-10 12:14:32,487 - INFO - Epoch 37/100 | Train Loss: 0.183735 | Val Loss: 0.188004\n",
      "2025-05-10 12:14:32,754 - INFO - Epoch 38/100 | Train Loss: 0.191961 | Val Loss: 0.182382\n",
      "2025-05-10 12:14:32,999 - INFO - Epoch 39/100 | Train Loss: 0.180634 | Val Loss: 0.182902\n",
      "2025-05-10 12:14:33,271 - INFO - Epoch 40/100 | Train Loss: 0.195238 | Val Loss: 0.184134\n",
      "2025-05-10 12:14:33,534 - INFO - Epoch 41/100 | Train Loss: 0.181416 | Val Loss: 0.192676\n",
      "2025-05-10 12:14:33,814 - INFO - Epoch 42/100 | Train Loss: 0.179524 | Val Loss: 0.174084\n",
      "2025-05-10 12:14:34,074 - INFO - Epoch 43/100 | Train Loss: 0.178267 | Val Loss: 0.172827\n",
      "2025-05-10 12:14:34,358 - INFO - Epoch 44/100 | Train Loss: 0.181962 | Val Loss: 0.165906\n",
      "2025-05-10 12:14:34,618 - INFO - Epoch 45/100 | Train Loss: 0.171779 | Val Loss: 0.174806\n",
      "2025-05-10 12:14:34,877 - INFO - Epoch 46/100 | Train Loss: 0.180974 | Val Loss: 0.170973\n",
      "2025-05-10 12:14:35,152 - INFO - Epoch 47/100 | Train Loss: 0.162038 | Val Loss: 0.160337\n",
      "2025-05-10 12:14:35,433 - INFO - Epoch 48/100 | Train Loss: 0.165952 | Val Loss: 0.166110\n",
      "2025-05-10 12:14:35,702 - INFO - Epoch 49/100 | Train Loss: 0.178224 | Val Loss: 0.169162\n",
      "2025-05-10 12:14:35,970 - INFO - Epoch 50/100 | Train Loss: 0.166737 | Val Loss: 0.174912\n",
      "2025-05-10 12:14:36,228 - INFO - Epoch 51/100 | Train Loss: 0.178033 | Val Loss: 0.185185\n",
      "2025-05-10 12:14:36,519 - INFO - Epoch 52/100 | Train Loss: 0.179373 | Val Loss: 0.170003\n",
      "2025-05-10 12:14:36,779 - INFO - Epoch 53/100 | Train Loss: 0.175627 | Val Loss: 0.162818\n",
      "2025-05-10 12:14:37,035 - INFO - Epoch 54/100 | Train Loss: 0.167669 | Val Loss: 0.171161\n",
      "2025-05-10 12:14:37,291 - INFO - Epoch 55/100 | Train Loss: 0.172815 | Val Loss: 0.170386\n",
      "2025-05-10 12:14:37,581 - INFO - Epoch 56/100 | Train Loss: 0.162049 | Val Loss: 0.181388\n",
      "2025-05-10 12:14:37,843 - INFO - Epoch 57/100 | Train Loss: 0.168854 | Val Loss: 0.165447\n",
      "2025-05-10 12:14:37,844 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:14:37,845 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 16, 'learning_rate': 0.001, 'window_size': 96}\n",
      "2025-05-10 12:14:38,103 - INFO - Epoch 1/100 | Train Loss: 0.977929 | Val Loss: 0.926204\n",
      "2025-05-10 12:14:38,369 - INFO - Epoch 2/100 | Train Loss: 0.858714 | Val Loss: 0.784035\n",
      "2025-05-10 12:14:38,616 - INFO - Epoch 3/100 | Train Loss: 0.741421 | Val Loss: 0.701019\n",
      "2025-05-10 12:14:38,861 - INFO - Epoch 4/100 | Train Loss: 0.667577 | Val Loss: 0.643912\n",
      "2025-05-10 12:14:39,123 - INFO - Epoch 5/100 | Train Loss: 0.623035 | Val Loss: 0.610987\n",
      "2025-05-10 12:14:39,359 - INFO - Epoch 6/100 | Train Loss: 0.586572 | Val Loss: 0.584233\n",
      "2025-05-10 12:14:39,651 - INFO - Epoch 7/100 | Train Loss: 0.565263 | Val Loss: 0.555300\n",
      "2025-05-10 12:14:39,944 - INFO - Epoch 8/100 | Train Loss: 0.544401 | Val Loss: 0.538312\n",
      "2025-05-10 12:14:40,287 - INFO - Epoch 9/100 | Train Loss: 0.526489 | Val Loss: 0.515980\n",
      "2025-05-10 12:14:40,593 - INFO - Epoch 10/100 | Train Loss: 0.509745 | Val Loss: 0.508893\n",
      "2025-05-10 12:14:40,939 - INFO - Epoch 11/100 | Train Loss: 0.506033 | Val Loss: 0.486472\n",
      "2025-05-10 12:14:41,243 - INFO - Epoch 12/100 | Train Loss: 0.495025 | Val Loss: 0.480272\n",
      "2025-05-10 12:14:41,514 - INFO - Epoch 13/100 | Train Loss: 0.484798 | Val Loss: 0.481370\n",
      "2025-05-10 12:14:41,796 - INFO - Epoch 14/100 | Train Loss: 0.477388 | Val Loss: 0.485078\n",
      "2025-05-10 12:14:42,100 - INFO - Epoch 15/100 | Train Loss: 0.473606 | Val Loss: 0.470069\n",
      "2025-05-10 12:14:42,369 - INFO - Epoch 16/100 | Train Loss: 0.469825 | Val Loss: 0.474197\n",
      "2025-05-10 12:14:42,634 - INFO - Epoch 17/100 | Train Loss: 0.463058 | Val Loss: 0.468606\n",
      "2025-05-10 12:14:42,916 - INFO - Epoch 18/100 | Train Loss: 0.460577 | Val Loss: 0.462696\n",
      "2025-05-10 12:14:43,213 - INFO - Epoch 19/100 | Train Loss: 0.456246 | Val Loss: 0.447858\n",
      "2025-05-10 12:14:43,479 - INFO - Epoch 20/100 | Train Loss: 0.459910 | Val Loss: 0.447819\n",
      "2025-05-10 12:14:43,736 - INFO - Epoch 21/100 | Train Loss: 0.460398 | Val Loss: 0.451239\n",
      "2025-05-10 12:14:43,999 - INFO - Epoch 22/100 | Train Loss: 0.453845 | Val Loss: 0.440685\n",
      "2025-05-10 12:14:44,304 - INFO - Epoch 23/100 | Train Loss: 0.448535 | Val Loss: 0.459823\n",
      "2025-05-10 12:14:44,570 - INFO - Epoch 24/100 | Train Loss: 0.453399 | Val Loss: 0.448264\n",
      "2025-05-10 12:14:44,844 - INFO - Epoch 25/100 | Train Loss: 0.445864 | Val Loss: 0.449319\n",
      "2025-05-10 12:14:45,147 - INFO - Epoch 26/100 | Train Loss: 0.446687 | Val Loss: 0.441505\n",
      "2025-05-10 12:14:45,440 - INFO - Epoch 27/100 | Train Loss: 0.449773 | Val Loss: 0.438120\n",
      "2025-05-10 12:14:45,701 - INFO - Epoch 28/100 | Train Loss: 0.448654 | Val Loss: 0.453239\n",
      "2025-05-10 12:14:45,989 - INFO - Epoch 29/100 | Train Loss: 0.450838 | Val Loss: 0.446579\n",
      "2025-05-10 12:14:46,230 - INFO - Epoch 30/100 | Train Loss: 0.442937 | Val Loss: 0.444367\n",
      "2025-05-10 12:14:46,497 - INFO - Epoch 31/100 | Train Loss: 0.449754 | Val Loss: 0.450091\n",
      "2025-05-10 12:14:46,764 - INFO - Epoch 32/100 | Train Loss: 0.449413 | Val Loss: 0.443310\n",
      "2025-05-10 12:14:47,027 - INFO - Epoch 33/100 | Train Loss: 0.450264 | Val Loss: 0.443121\n",
      "2025-05-10 12:14:47,275 - INFO - Epoch 34/100 | Train Loss: 0.451625 | Val Loss: 0.439608\n",
      "2025-05-10 12:14:47,551 - INFO - Epoch 35/100 | Train Loss: 0.444177 | Val Loss: 0.450808\n",
      "2025-05-10 12:14:47,819 - INFO - Epoch 36/100 | Train Loss: 0.447876 | Val Loss: 0.440259\n",
      "2025-05-10 12:14:48,117 - INFO - Epoch 37/100 | Train Loss: 0.445540 | Val Loss: 0.445164\n",
      "2025-05-10 12:14:48,118 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:14:48,119 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 16, 'learning_rate': 0.0005, 'window_size': 64}\n",
      "2025-05-10 12:14:48,398 - INFO - Epoch 1/100 | Train Loss: 0.984349 | Val Loss: 0.956838\n",
      "2025-05-10 12:14:48,671 - INFO - Epoch 2/100 | Train Loss: 0.902133 | Val Loss: 0.835827\n",
      "2025-05-10 12:14:48,940 - INFO - Epoch 3/100 | Train Loss: 0.776448 | Val Loss: 0.727448\n",
      "2025-05-10 12:14:49,224 - INFO - Epoch 4/100 | Train Loss: 0.673807 | Val Loss: 0.634419\n",
      "2025-05-10 12:14:49,542 - INFO - Epoch 5/100 | Train Loss: 0.604187 | Val Loss: 0.575133\n",
      "2025-05-10 12:14:49,770 - INFO - Epoch 6/100 | Train Loss: 0.553132 | Val Loss: 0.531533\n",
      "2025-05-10 12:14:50,023 - INFO - Epoch 7/100 | Train Loss: 0.503304 | Val Loss: 0.490713\n",
      "2025-05-10 12:14:50,331 - INFO - Epoch 8/100 | Train Loss: 0.475629 | Val Loss: 0.447019\n",
      "2025-05-10 12:14:50,583 - INFO - Epoch 9/100 | Train Loss: 0.436532 | Val Loss: 0.434063\n",
      "2025-05-10 12:14:50,850 - INFO - Epoch 10/100 | Train Loss: 0.418098 | Val Loss: 0.405543\n",
      "2025-05-10 12:14:51,129 - INFO - Epoch 11/100 | Train Loss: 0.402485 | Val Loss: 0.383579\n",
      "2025-05-10 12:14:51,424 - INFO - Epoch 12/100 | Train Loss: 0.374981 | Val Loss: 0.371874\n",
      "2025-05-10 12:14:51,685 - INFO - Epoch 13/100 | Train Loss: 0.355604 | Val Loss: 0.339619\n",
      "2025-05-10 12:14:51,939 - INFO - Epoch 14/100 | Train Loss: 0.343227 | Val Loss: 0.340639\n",
      "2025-05-10 12:14:52,196 - INFO - Epoch 15/100 | Train Loss: 0.327875 | Val Loss: 0.323192\n",
      "2025-05-10 12:14:52,503 - INFO - Epoch 16/100 | Train Loss: 0.318699 | Val Loss: 0.317488\n",
      "2025-05-10 12:14:52,766 - INFO - Epoch 17/100 | Train Loss: 0.315838 | Val Loss: 0.302701\n",
      "2025-05-10 12:14:53,017 - INFO - Epoch 18/100 | Train Loss: 0.309129 | Val Loss: 0.294904\n",
      "2025-05-10 12:14:53,278 - INFO - Epoch 19/100 | Train Loss: 0.286626 | Val Loss: 0.283087\n",
      "2025-05-10 12:14:53,562 - INFO - Epoch 20/100 | Train Loss: 0.283888 | Val Loss: 0.273320\n",
      "2025-05-10 12:14:53,831 - INFO - Epoch 21/100 | Train Loss: 0.276979 | Val Loss: 0.280117\n",
      "2025-05-10 12:14:54,148 - INFO - Epoch 22/100 | Train Loss: 0.271138 | Val Loss: 0.260248\n",
      "2025-05-10 12:14:54,432 - INFO - Epoch 23/100 | Train Loss: 0.263439 | Val Loss: 0.258308\n",
      "2025-05-10 12:14:54,695 - INFO - Epoch 24/100 | Train Loss: 0.259771 | Val Loss: 0.251512\n",
      "2025-05-10 12:14:54,934 - INFO - Epoch 25/100 | Train Loss: 0.256460 | Val Loss: 0.258992\n",
      "2025-05-10 12:14:55,227 - INFO - Epoch 26/100 | Train Loss: 0.243190 | Val Loss: 0.234916\n",
      "2025-05-10 12:14:55,494 - INFO - Epoch 27/100 | Train Loss: 0.240171 | Val Loss: 0.232961\n",
      "2025-05-10 12:14:55,729 - INFO - Epoch 28/100 | Train Loss: 0.243157 | Val Loss: 0.236377\n",
      "2025-05-10 12:14:55,967 - INFO - Epoch 29/100 | Train Loss: 0.229221 | Val Loss: 0.213461\n",
      "2025-05-10 12:14:56,301 - INFO - Epoch 30/100 | Train Loss: 0.231051 | Val Loss: 0.234718\n",
      "2025-05-10 12:14:56,622 - INFO - Epoch 31/100 | Train Loss: 0.232858 | Val Loss: 0.218865\n",
      "2025-05-10 12:14:56,952 - INFO - Epoch 32/100 | Train Loss: 0.219925 | Val Loss: 0.229127\n",
      "2025-05-10 12:14:57,265 - INFO - Epoch 33/100 | Train Loss: 0.222762 | Val Loss: 0.225558\n",
      "2025-05-10 12:14:57,572 - INFO - Epoch 34/100 | Train Loss: 0.221678 | Val Loss: 0.215527\n",
      "2025-05-10 12:14:57,854 - INFO - Epoch 35/100 | Train Loss: 0.221714 | Val Loss: 0.226283\n",
      "2025-05-10 12:14:58,121 - INFO - Epoch 36/100 | Train Loss: 0.213527 | Val Loss: 0.210373\n",
      "2025-05-10 12:14:58,383 - INFO - Epoch 37/100 | Train Loss: 0.218554 | Val Loss: 0.223503\n",
      "2025-05-10 12:14:58,669 - INFO - Epoch 38/100 | Train Loss: 0.218821 | Val Loss: 0.223746\n",
      "2025-05-10 12:14:58,976 - INFO - Epoch 39/100 | Train Loss: 0.201163 | Val Loss: 0.215520\n",
      "2025-05-10 12:14:59,243 - INFO - Epoch 40/100 | Train Loss: 0.203454 | Val Loss: 0.226479\n",
      "2025-05-10 12:14:59,521 - INFO - Epoch 41/100 | Train Loss: 0.206994 | Val Loss: 0.203847\n",
      "2025-05-10 12:14:59,843 - INFO - Epoch 42/100 | Train Loss: 0.205943 | Val Loss: 0.196706\n",
      "2025-05-10 12:15:00,138 - INFO - Epoch 43/100 | Train Loss: 0.195566 | Val Loss: 0.224657\n",
      "2025-05-10 12:15:00,387 - INFO - Epoch 44/100 | Train Loss: 0.205859 | Val Loss: 0.199008\n",
      "2025-05-10 12:15:00,630 - INFO - Epoch 45/100 | Train Loss: 0.199316 | Val Loss: 0.194688\n",
      "2025-05-10 12:15:00,915 - INFO - Epoch 46/100 | Train Loss: 0.191157 | Val Loss: 0.198296\n",
      "2025-05-10 12:15:01,191 - INFO - Epoch 47/100 | Train Loss: 0.197762 | Val Loss: 0.200858\n",
      "2025-05-10 12:15:01,474 - INFO - Epoch 48/100 | Train Loss: 0.190109 | Val Loss: 0.186005\n",
      "2025-05-10 12:15:01,761 - INFO - Epoch 49/100 | Train Loss: 0.185306 | Val Loss: 0.199496\n",
      "2025-05-10 12:15:02,032 - INFO - Epoch 50/100 | Train Loss: 0.191432 | Val Loss: 0.167338\n",
      "2025-05-10 12:15:02,285 - INFO - Epoch 51/100 | Train Loss: 0.185005 | Val Loss: 0.199984\n",
      "2025-05-10 12:15:02,541 - INFO - Epoch 52/100 | Train Loss: 0.187319 | Val Loss: 0.187127\n",
      "2025-05-10 12:15:02,838 - INFO - Epoch 53/100 | Train Loss: 0.181148 | Val Loss: 0.175930\n",
      "2025-05-10 12:15:03,109 - INFO - Epoch 54/100 | Train Loss: 0.178838 | Val Loss: 0.192671\n",
      "2025-05-10 12:15:03,422 - INFO - Epoch 55/100 | Train Loss: 0.180658 | Val Loss: 0.189001\n",
      "2025-05-10 12:15:03,740 - INFO - Epoch 56/100 | Train Loss: 0.182912 | Val Loss: 0.185136\n",
      "2025-05-10 12:15:04,002 - INFO - Epoch 57/100 | Train Loss: 0.175032 | Val Loss: 0.177450\n",
      "2025-05-10 12:15:04,300 - INFO - Epoch 58/100 | Train Loss: 0.175686 | Val Loss: 0.168045\n",
      "2025-05-10 12:15:04,569 - INFO - Epoch 59/100 | Train Loss: 0.180081 | Val Loss: 0.166765\n",
      "2025-05-10 12:15:04,852 - INFO - Epoch 60/100 | Train Loss: 0.169602 | Val Loss: 0.154934\n",
      "2025-05-10 12:15:05,091 - INFO - Epoch 61/100 | Train Loss: 0.166378 | Val Loss: 0.165781\n",
      "2025-05-10 12:15:05,397 - INFO - Epoch 62/100 | Train Loss: 0.181609 | Val Loss: 0.159843\n",
      "2025-05-10 12:15:05,695 - INFO - Epoch 63/100 | Train Loss: 0.172738 | Val Loss: 0.158697\n",
      "2025-05-10 12:15:05,957 - INFO - Epoch 64/100 | Train Loss: 0.170059 | Val Loss: 0.156960\n",
      "2025-05-10 12:15:06,196 - INFO - Epoch 65/100 | Train Loss: 0.162431 | Val Loss: 0.177592\n",
      "2025-05-10 12:15:06,513 - INFO - Epoch 66/100 | Train Loss: 0.163320 | Val Loss: 0.180485\n",
      "2025-05-10 12:15:06,785 - INFO - Epoch 67/100 | Train Loss: 0.170901 | Val Loss: 0.161083\n",
      "2025-05-10 12:15:07,056 - INFO - Epoch 68/100 | Train Loss: 0.164128 | Val Loss: 0.172131\n",
      "2025-05-10 12:15:07,303 - INFO - Epoch 69/100 | Train Loss: 0.164889 | Val Loss: 0.179076\n",
      "2025-05-10 12:15:07,588 - INFO - Epoch 70/100 | Train Loss: 0.170950 | Val Loss: 0.161823\n",
      "2025-05-10 12:15:07,589 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:15:07,591 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 16, 'learning_rate': 0.0005, 'window_size': 96}\n",
      "2025-05-10 12:15:07,911 - INFO - Epoch 1/100 | Train Loss: 0.994891 | Val Loss: 0.970046\n",
      "2025-05-10 12:15:08,202 - INFO - Epoch 2/100 | Train Loss: 0.938261 | Val Loss: 0.902120\n",
      "2025-05-10 12:15:08,495 - INFO - Epoch 3/100 | Train Loss: 0.857592 | Val Loss: 0.816991\n",
      "2025-05-10 12:15:08,760 - INFO - Epoch 4/100 | Train Loss: 0.787133 | Val Loss: 0.757794\n",
      "2025-05-10 12:15:08,999 - INFO - Epoch 5/100 | Train Loss: 0.733704 | Val Loss: 0.714374\n",
      "2025-05-10 12:15:09,271 - INFO - Epoch 6/100 | Train Loss: 0.690416 | Val Loss: 0.671169\n",
      "2025-05-10 12:15:09,561 - INFO - Epoch 7/100 | Train Loss: 0.658062 | Val Loss: 0.648523\n",
      "2025-05-10 12:15:09,852 - INFO - Epoch 8/100 | Train Loss: 0.633979 | Val Loss: 0.620163\n",
      "2025-05-10 12:15:10,152 - INFO - Epoch 9/100 | Train Loss: 0.609828 | Val Loss: 0.590159\n",
      "2025-05-10 12:15:10,439 - INFO - Epoch 10/100 | Train Loss: 0.584951 | Val Loss: 0.577464\n",
      "2025-05-10 12:15:10,689 - INFO - Epoch 11/100 | Train Loss: 0.571704 | Val Loss: 0.568071\n",
      "2025-05-10 12:15:10,949 - INFO - Epoch 12/100 | Train Loss: 0.560336 | Val Loss: 0.554736\n",
      "2025-05-10 12:15:11,183 - INFO - Epoch 13/100 | Train Loss: 0.550929 | Val Loss: 0.552084\n",
      "2025-05-10 12:15:11,457 - INFO - Epoch 14/100 | Train Loss: 0.536337 | Val Loss: 0.525709\n",
      "2025-05-10 12:15:11,701 - INFO - Epoch 15/100 | Train Loss: 0.528921 | Val Loss: 0.520541\n",
      "2025-05-10 12:15:11,965 - INFO - Epoch 16/100 | Train Loss: 0.521275 | Val Loss: 0.517321\n",
      "2025-05-10 12:15:12,242 - INFO - Epoch 17/100 | Train Loss: 0.512128 | Val Loss: 0.511857\n",
      "2025-05-10 12:15:12,532 - INFO - Epoch 18/100 | Train Loss: 0.500575 | Val Loss: 0.497956\n",
      "2025-05-10 12:15:12,896 - INFO - Epoch 19/100 | Train Loss: 0.504809 | Val Loss: 0.497885\n",
      "2025-05-10 12:15:13,225 - INFO - Epoch 20/100 | Train Loss: 0.497189 | Val Loss: 0.486402\n",
      "2025-05-10 12:15:13,515 - INFO - Epoch 21/100 | Train Loss: 0.490802 | Val Loss: 0.480602\n",
      "2025-05-10 12:15:13,849 - INFO - Epoch 22/100 | Train Loss: 0.480036 | Val Loss: 0.478391\n",
      "2025-05-10 12:15:14,120 - INFO - Epoch 23/100 | Train Loss: 0.481360 | Val Loss: 0.480456\n",
      "2025-05-10 12:15:14,406 - INFO - Epoch 24/100 | Train Loss: 0.476449 | Val Loss: 0.482195\n",
      "2025-05-10 12:15:14,665 - INFO - Epoch 25/100 | Train Loss: 0.469734 | Val Loss: 0.474818\n",
      "2025-05-10 12:15:14,992 - INFO - Epoch 26/100 | Train Loss: 0.468463 | Val Loss: 0.474039\n",
      "2025-05-10 12:15:15,299 - INFO - Epoch 27/100 | Train Loss: 0.465126 | Val Loss: 0.464870\n",
      "2025-05-10 12:15:15,588 - INFO - Epoch 28/100 | Train Loss: 0.467167 | Val Loss: 0.468686\n",
      "2025-05-10 12:15:15,831 - INFO - Epoch 29/100 | Train Loss: 0.458227 | Val Loss: 0.468720\n",
      "2025-05-10 12:15:16,116 - INFO - Epoch 30/100 | Train Loss: 0.467827 | Val Loss: 0.466542\n",
      "2025-05-10 12:15:16,357 - INFO - Epoch 31/100 | Train Loss: 0.461522 | Val Loss: 0.470712\n",
      "2025-05-10 12:15:16,632 - INFO - Epoch 32/100 | Train Loss: 0.459398 | Val Loss: 0.459652\n",
      "2025-05-10 12:15:16,901 - INFO - Epoch 33/100 | Train Loss: 0.457246 | Val Loss: 0.473403\n",
      "2025-05-10 12:15:17,213 - INFO - Epoch 34/100 | Train Loss: 0.462800 | Val Loss: 0.465593\n",
      "2025-05-10 12:15:17,485 - INFO - Epoch 35/100 | Train Loss: 0.451755 | Val Loss: 0.457124\n",
      "2025-05-10 12:15:17,772 - INFO - Epoch 36/100 | Train Loss: 0.457834 | Val Loss: 0.455478\n",
      "2025-05-10 12:15:18,037 - INFO - Epoch 37/100 | Train Loss: 0.453705 | Val Loss: 0.450555\n",
      "2025-05-10 12:15:18,326 - INFO - Epoch 38/100 | Train Loss: 0.449128 | Val Loss: 0.448063\n",
      "2025-05-10 12:15:18,608 - INFO - Epoch 39/100 | Train Loss: 0.453923 | Val Loss: 0.456587\n",
      "2025-05-10 12:15:18,894 - INFO - Epoch 40/100 | Train Loss: 0.451128 | Val Loss: 0.452881\n",
      "2025-05-10 12:15:19,161 - INFO - Epoch 41/100 | Train Loss: 0.455240 | Val Loss: 0.456379\n",
      "2025-05-10 12:15:19,430 - INFO - Epoch 42/100 | Train Loss: 0.450155 | Val Loss: 0.446351\n",
      "2025-05-10 12:15:19,708 - INFO - Epoch 43/100 | Train Loss: 0.446557 | Val Loss: 0.465391\n",
      "2025-05-10 12:15:20,004 - INFO - Epoch 44/100 | Train Loss: 0.444881 | Val Loss: 0.449482\n",
      "2025-05-10 12:15:20,341 - INFO - Epoch 45/100 | Train Loss: 0.445238 | Val Loss: 0.441300\n",
      "2025-05-10 12:15:20,639 - INFO - Epoch 46/100 | Train Loss: 0.443638 | Val Loss: 0.442881\n",
      "2025-05-10 12:15:20,923 - INFO - Epoch 47/100 | Train Loss: 0.444222 | Val Loss: 0.441480\n",
      "2025-05-10 12:15:21,203 - INFO - Epoch 48/100 | Train Loss: 0.441822 | Val Loss: 0.441979\n",
      "2025-05-10 12:15:21,473 - INFO - Epoch 49/100 | Train Loss: 0.446548 | Val Loss: 0.453126\n",
      "2025-05-10 12:15:21,750 - INFO - Epoch 50/100 | Train Loss: 0.448277 | Val Loss: 0.442548\n",
      "2025-05-10 12:15:22,010 - INFO - Epoch 51/100 | Train Loss: 0.440072 | Val Loss: 0.447268\n",
      "2025-05-10 12:15:22,312 - INFO - Epoch 52/100 | Train Loss: 0.447415 | Val Loss: 0.450542\n",
      "2025-05-10 12:15:22,581 - INFO - Epoch 53/100 | Train Loss: 0.444360 | Val Loss: 0.454972\n",
      "2025-05-10 12:15:22,860 - INFO - Epoch 54/100 | Train Loss: 0.444968 | Val Loss: 0.448903\n",
      "2025-05-10 12:15:23,151 - INFO - Epoch 55/100 | Train Loss: 0.446367 | Val Loss: 0.446992\n",
      "2025-05-10 12:15:23,153 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:15:23,154 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 32, 'learning_rate': 0.001, 'window_size': 64}\n",
      "2025-05-10 12:15:23,494 - INFO - Epoch 1/100 | Train Loss: 0.954541 | Val Loss: 0.853636\n",
      "2025-05-10 12:15:23,778 - INFO - Epoch 2/100 | Train Loss: 0.747186 | Val Loss: 0.650395\n",
      "2025-05-10 12:15:24,064 - INFO - Epoch 3/100 | Train Loss: 0.598005 | Val Loss: 0.548334\n",
      "2025-05-10 12:15:24,356 - INFO - Epoch 4/100 | Train Loss: 0.509977 | Val Loss: 0.469061\n",
      "2025-05-10 12:15:24,616 - INFO - Epoch 5/100 | Train Loss: 0.450061 | Val Loss: 0.425058\n",
      "2025-05-10 12:15:24,865 - INFO - Epoch 6/100 | Train Loss: 0.396006 | Val Loss: 0.397780\n",
      "2025-05-10 12:15:25,129 - INFO - Epoch 7/100 | Train Loss: 0.375887 | Val Loss: 0.359568\n",
      "2025-05-10 12:15:25,439 - INFO - Epoch 8/100 | Train Loss: 0.356264 | Val Loss: 0.330063\n",
      "2025-05-10 12:15:25,707 - INFO - Epoch 9/100 | Train Loss: 0.322379 | Val Loss: 0.312377\n",
      "2025-05-10 12:15:25,981 - INFO - Epoch 10/100 | Train Loss: 0.306729 | Val Loss: 0.301218\n",
      "2025-05-10 12:15:26,280 - INFO - Epoch 11/100 | Train Loss: 0.286715 | Val Loss: 0.291524\n",
      "2025-05-10 12:15:26,579 - INFO - Epoch 12/100 | Train Loss: 0.284090 | Val Loss: 0.279959\n",
      "2025-05-10 12:15:26,848 - INFO - Epoch 13/100 | Train Loss: 0.271199 | Val Loss: 0.261836\n",
      "2025-05-10 12:15:27,136 - INFO - Epoch 14/100 | Train Loss: 0.266823 | Val Loss: 0.262139\n",
      "2025-05-10 12:15:27,381 - INFO - Epoch 15/100 | Train Loss: 0.264210 | Val Loss: 0.249231\n",
      "2025-05-10 12:15:27,643 - INFO - Epoch 16/100 | Train Loss: 0.264436 | Val Loss: 0.235468\n",
      "2025-05-10 12:15:27,873 - INFO - Epoch 17/100 | Train Loss: 0.237691 | Val Loss: 0.252073\n",
      "2025-05-10 12:15:28,114 - INFO - Epoch 18/100 | Train Loss: 0.241701 | Val Loss: 0.238681\n",
      "2025-05-10 12:15:28,442 - INFO - Epoch 19/100 | Train Loss: 0.228246 | Val Loss: 0.231249\n",
      "2025-05-10 12:15:28,667 - INFO - Epoch 20/100 | Train Loss: 0.233124 | Val Loss: 0.216680\n",
      "2025-05-10 12:15:28,932 - INFO - Epoch 21/100 | Train Loss: 0.229014 | Val Loss: 0.224560\n",
      "2025-05-10 12:15:29,197 - INFO - Epoch 22/100 | Train Loss: 0.227898 | Val Loss: 0.226859\n",
      "2025-05-10 12:15:29,483 - INFO - Epoch 23/100 | Train Loss: 0.219207 | Val Loss: 0.209402\n",
      "2025-05-10 12:15:29,763 - INFO - Epoch 24/100 | Train Loss: 0.217133 | Val Loss: 0.226214\n",
      "2025-05-10 12:15:30,023 - INFO - Epoch 25/100 | Train Loss: 0.218657 | Val Loss: 0.213515\n",
      "2025-05-10 12:15:30,260 - INFO - Epoch 26/100 | Train Loss: 0.211022 | Val Loss: 0.206863\n",
      "2025-05-10 12:15:30,492 - INFO - Epoch 27/100 | Train Loss: 0.209832 | Val Loss: 0.213565\n",
      "2025-05-10 12:15:30,757 - INFO - Epoch 28/100 | Train Loss: 0.206755 | Val Loss: 0.214067\n",
      "2025-05-10 12:15:31,037 - INFO - Epoch 29/100 | Train Loss: 0.209776 | Val Loss: 0.201499\n",
      "2025-05-10 12:15:31,275 - INFO - Epoch 30/100 | Train Loss: 0.208610 | Val Loss: 0.208030\n",
      "2025-05-10 12:15:31,532 - INFO - Epoch 31/100 | Train Loss: 0.201029 | Val Loss: 0.214170\n",
      "2025-05-10 12:15:31,775 - INFO - Epoch 32/100 | Train Loss: 0.199526 | Val Loss: 0.190245\n",
      "2025-05-10 12:15:32,007 - INFO - Epoch 33/100 | Train Loss: 0.187813 | Val Loss: 0.179411\n",
      "2025-05-10 12:15:32,248 - INFO - Epoch 34/100 | Train Loss: 0.196652 | Val Loss: 0.192994\n",
      "2025-05-10 12:15:32,498 - INFO - Epoch 35/100 | Train Loss: 0.198565 | Val Loss: 0.181165\n",
      "2025-05-10 12:15:32,743 - INFO - Epoch 36/100 | Train Loss: 0.197699 | Val Loss: 0.199384\n",
      "2025-05-10 12:15:32,959 - INFO - Epoch 37/100 | Train Loss: 0.189669 | Val Loss: 0.210282\n",
      "2025-05-10 12:15:33,203 - INFO - Epoch 38/100 | Train Loss: 0.195014 | Val Loss: 0.200635\n",
      "2025-05-10 12:15:33,433 - INFO - Epoch 39/100 | Train Loss: 0.195874 | Val Loss: 0.195023\n",
      "2025-05-10 12:15:33,655 - INFO - Epoch 40/100 | Train Loss: 0.194001 | Val Loss: 0.185671\n",
      "2025-05-10 12:15:33,894 - INFO - Epoch 41/100 | Train Loss: 0.197321 | Val Loss: 0.199861\n",
      "2025-05-10 12:15:34,145 - INFO - Epoch 42/100 | Train Loss: 0.195817 | Val Loss: 0.189051\n",
      "2025-05-10 12:15:34,369 - INFO - Epoch 43/100 | Train Loss: 0.187877 | Val Loss: 0.202281\n",
      "2025-05-10 12:15:34,370 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:15:34,371 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 32, 'learning_rate': 0.001, 'window_size': 96}\n",
      "2025-05-10 12:15:34,605 - INFO - Epoch 1/100 | Train Loss: 0.976150 | Val Loss: 0.919209\n",
      "2025-05-10 12:15:34,853 - INFO - Epoch 2/100 | Train Loss: 0.842526 | Val Loss: 0.772019\n",
      "2025-05-10 12:15:35,079 - INFO - Epoch 3/100 | Train Loss: 0.730152 | Val Loss: 0.690654\n",
      "2025-05-10 12:15:35,305 - INFO - Epoch 4/100 | Train Loss: 0.666568 | Val Loss: 0.643993\n",
      "2025-05-10 12:15:35,585 - INFO - Epoch 5/100 | Train Loss: 0.622147 | Val Loss: 0.602717\n",
      "2025-05-10 12:15:35,818 - INFO - Epoch 6/100 | Train Loss: 0.588362 | Val Loss: 0.581843\n",
      "2025-05-10 12:15:36,055 - INFO - Epoch 7/100 | Train Loss: 0.569358 | Val Loss: 0.565725\n",
      "2025-05-10 12:15:36,279 - INFO - Epoch 8/100 | Train Loss: 0.541414 | Val Loss: 0.529964\n",
      "2025-05-10 12:15:36,502 - INFO - Epoch 9/100 | Train Loss: 0.533106 | Val Loss: 0.526427\n",
      "2025-05-10 12:15:36,720 - INFO - Epoch 10/100 | Train Loss: 0.514635 | Val Loss: 0.512670\n",
      "2025-05-10 12:15:36,970 - INFO - Epoch 11/100 | Train Loss: 0.508256 | Val Loss: 0.500516\n",
      "2025-05-10 12:15:37,209 - INFO - Epoch 12/100 | Train Loss: 0.502381 | Val Loss: 0.490315\n",
      "2025-05-10 12:15:37,448 - INFO - Epoch 13/100 | Train Loss: 0.496533 | Val Loss: 0.497416\n",
      "2025-05-10 12:15:37,692 - INFO - Epoch 14/100 | Train Loss: 0.485528 | Val Loss: 0.482193\n",
      "2025-05-10 12:15:37,937 - INFO - Epoch 15/100 | Train Loss: 0.477574 | Val Loss: 0.478131\n",
      "2025-05-10 12:15:38,185 - INFO - Epoch 16/100 | Train Loss: 0.478193 | Val Loss: 0.490958\n",
      "2025-05-10 12:15:38,422 - INFO - Epoch 17/100 | Train Loss: 0.475854 | Val Loss: 0.477712\n",
      "2025-05-10 12:15:38,676 - INFO - Epoch 18/100 | Train Loss: 0.469403 | Val Loss: 0.470587\n",
      "2025-05-10 12:15:38,908 - INFO - Epoch 19/100 | Train Loss: 0.477671 | Val Loss: 0.476783\n",
      "2025-05-10 12:15:39,149 - INFO - Epoch 20/100 | Train Loss: 0.470187 | Val Loss: 0.479264\n",
      "2025-05-10 12:15:39,408 - INFO - Epoch 21/100 | Train Loss: 0.463636 | Val Loss: 0.460550\n",
      "2025-05-10 12:15:39,644 - INFO - Epoch 22/100 | Train Loss: 0.466445 | Val Loss: 0.480095\n",
      "2025-05-10 12:15:39,871 - INFO - Epoch 23/100 | Train Loss: 0.469030 | Val Loss: 0.464099\n",
      "2025-05-10 12:15:40,181 - INFO - Epoch 24/100 | Train Loss: 0.461625 | Val Loss: 0.473852\n",
      "2025-05-10 12:15:40,455 - INFO - Epoch 25/100 | Train Loss: 0.467002 | Val Loss: 0.464798\n",
      "2025-05-10 12:15:40,677 - INFO - Epoch 26/100 | Train Loss: 0.464388 | Val Loss: 0.455875\n",
      "2025-05-10 12:15:40,933 - INFO - Epoch 27/100 | Train Loss: 0.462713 | Val Loss: 0.469640\n",
      "2025-05-10 12:15:41,167 - INFO - Epoch 28/100 | Train Loss: 0.462198 | Val Loss: 0.479231\n",
      "2025-05-10 12:15:41,419 - INFO - Epoch 29/100 | Train Loss: 0.465453 | Val Loss: 0.465902\n",
      "2025-05-10 12:15:41,651 - INFO - Epoch 30/100 | Train Loss: 0.466285 | Val Loss: 0.467642\n",
      "2025-05-10 12:15:41,939 - INFO - Epoch 31/100 | Train Loss: 0.463802 | Val Loss: 0.467380\n",
      "2025-05-10 12:15:42,171 - INFO - Epoch 32/100 | Train Loss: 0.463720 | Val Loss: 0.464312\n",
      "2025-05-10 12:15:42,414 - INFO - Epoch 33/100 | Train Loss: 0.467607 | Val Loss: 0.476604\n",
      "2025-05-10 12:15:42,680 - INFO - Epoch 34/100 | Train Loss: 0.461326 | Val Loss: 0.469321\n",
      "2025-05-10 12:15:42,911 - INFO - Epoch 35/100 | Train Loss: 0.461438 | Val Loss: 0.474882\n",
      "2025-05-10 12:15:43,146 - INFO - Epoch 36/100 | Train Loss: 0.461912 | Val Loss: 0.459178\n",
      "2025-05-10 12:15:43,148 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:15:43,148 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 32, 'learning_rate': 0.0005, 'window_size': 64}\n",
      "2025-05-10 12:15:43,386 - INFO - Epoch 1/100 | Train Loss: 0.984955 | Val Loss: 0.953947\n",
      "2025-05-10 12:15:43,627 - INFO - Epoch 2/100 | Train Loss: 0.900101 | Val Loss: 0.838202\n",
      "2025-05-10 12:15:43,858 - INFO - Epoch 3/100 | Train Loss: 0.769397 | Val Loss: 0.719780\n",
      "2025-05-10 12:15:44,119 - INFO - Epoch 4/100 | Train Loss: 0.670557 | Val Loss: 0.630140\n",
      "2025-05-10 12:15:44,420 - INFO - Epoch 5/100 | Train Loss: 0.603600 | Val Loss: 0.576947\n",
      "2025-05-10 12:15:44,720 - INFO - Epoch 6/100 | Train Loss: 0.543709 | Val Loss: 0.522888\n",
      "2025-05-10 12:15:45,020 - INFO - Epoch 7/100 | Train Loss: 0.500268 | Val Loss: 0.485276\n",
      "2025-05-10 12:15:45,306 - INFO - Epoch 8/100 | Train Loss: 0.462438 | Val Loss: 0.446567\n",
      "2025-05-10 12:15:45,595 - INFO - Epoch 9/100 | Train Loss: 0.436143 | Val Loss: 0.406261\n",
      "2025-05-10 12:15:45,843 - INFO - Epoch 10/100 | Train Loss: 0.407969 | Val Loss: 0.406139\n",
      "2025-05-10 12:15:46,090 - INFO - Epoch 11/100 | Train Loss: 0.382729 | Val Loss: 0.378869\n",
      "2025-05-10 12:15:46,324 - INFO - Epoch 12/100 | Train Loss: 0.373277 | Val Loss: 0.379567\n",
      "2025-05-10 12:15:46,575 - INFO - Epoch 13/100 | Train Loss: 0.353645 | Val Loss: 0.348109\n",
      "2025-05-10 12:15:46,802 - INFO - Epoch 14/100 | Train Loss: 0.347648 | Val Loss: 0.331990\n",
      "2025-05-10 12:15:47,035 - INFO - Epoch 15/100 | Train Loss: 0.336135 | Val Loss: 0.320207\n",
      "2025-05-10 12:15:47,269 - INFO - Epoch 16/100 | Train Loss: 0.318131 | Val Loss: 0.309324\n",
      "2025-05-10 12:15:47,513 - INFO - Epoch 17/100 | Train Loss: 0.307152 | Val Loss: 0.301023\n",
      "2025-05-10 12:15:47,762 - INFO - Epoch 18/100 | Train Loss: 0.298838 | Val Loss: 0.298030\n",
      "2025-05-10 12:15:48,007 - INFO - Epoch 19/100 | Train Loss: 0.294249 | Val Loss: 0.292929\n",
      "2025-05-10 12:15:48,255 - INFO - Epoch 20/100 | Train Loss: 0.288589 | Val Loss: 0.285388\n",
      "2025-05-10 12:15:48,539 - INFO - Epoch 21/100 | Train Loss: 0.280931 | Val Loss: 0.272432\n",
      "2025-05-10 12:15:48,795 - INFO - Epoch 22/100 | Train Loss: 0.272448 | Val Loss: 0.276971\n",
      "2025-05-10 12:15:49,031 - INFO - Epoch 23/100 | Train Loss: 0.265195 | Val Loss: 0.271768\n",
      "2025-05-10 12:15:49,254 - INFO - Epoch 24/100 | Train Loss: 0.259403 | Val Loss: 0.266114\n",
      "2025-05-10 12:15:49,484 - INFO - Epoch 25/100 | Train Loss: 0.260350 | Val Loss: 0.263114\n",
      "2025-05-10 12:15:49,766 - INFO - Epoch 26/100 | Train Loss: 0.256938 | Val Loss: 0.256578\n",
      "2025-05-10 12:15:50,032 - INFO - Epoch 27/100 | Train Loss: 0.253894 | Val Loss: 0.237892\n",
      "2025-05-10 12:15:50,281 - INFO - Epoch 28/100 | Train Loss: 0.252115 | Val Loss: 0.245409\n",
      "2025-05-10 12:15:50,528 - INFO - Epoch 29/100 | Train Loss: 0.254645 | Val Loss: 0.245412\n",
      "2025-05-10 12:15:50,781 - INFO - Epoch 30/100 | Train Loss: 0.238805 | Val Loss: 0.235818\n",
      "2025-05-10 12:15:51,030 - INFO - Epoch 31/100 | Train Loss: 0.234230 | Val Loss: 0.244279\n",
      "2025-05-10 12:15:51,284 - INFO - Epoch 32/100 | Train Loss: 0.239817 | Val Loss: 0.228243\n",
      "2025-05-10 12:15:51,500 - INFO - Epoch 33/100 | Train Loss: 0.236106 | Val Loss: 0.225892\n",
      "2025-05-10 12:15:51,740 - INFO - Epoch 34/100 | Train Loss: 0.233522 | Val Loss: 0.223799\n",
      "2025-05-10 12:15:51,994 - INFO - Epoch 35/100 | Train Loss: 0.218784 | Val Loss: 0.216417\n",
      "2025-05-10 12:15:52,221 - INFO - Epoch 36/100 | Train Loss: 0.221683 | Val Loss: 0.226717\n",
      "2025-05-10 12:15:52,468 - INFO - Epoch 37/100 | Train Loss: 0.214737 | Val Loss: 0.215871\n",
      "2025-05-10 12:15:52,729 - INFO - Epoch 38/100 | Train Loss: 0.214286 | Val Loss: 0.220900\n",
      "2025-05-10 12:15:52,957 - INFO - Epoch 39/100 | Train Loss: 0.219252 | Val Loss: 0.221214\n",
      "2025-05-10 12:15:53,215 - INFO - Epoch 40/100 | Train Loss: 0.211639 | Val Loss: 0.206335\n",
      "2025-05-10 12:15:53,470 - INFO - Epoch 41/100 | Train Loss: 0.212371 | Val Loss: 0.213314\n",
      "2025-05-10 12:15:53,729 - INFO - Epoch 42/100 | Train Loss: 0.206529 | Val Loss: 0.218793\n",
      "2025-05-10 12:15:53,975 - INFO - Epoch 43/100 | Train Loss: 0.198397 | Val Loss: 0.209013\n",
      "2025-05-10 12:15:54,217 - INFO - Epoch 44/100 | Train Loss: 0.214504 | Val Loss: 0.196802\n",
      "2025-05-10 12:15:54,476 - INFO - Epoch 45/100 | Train Loss: 0.197503 | Val Loss: 0.198749\n",
      "2025-05-10 12:15:54,730 - INFO - Epoch 46/100 | Train Loss: 0.203731 | Val Loss: 0.211690\n",
      "2025-05-10 12:15:54,971 - INFO - Epoch 47/100 | Train Loss: 0.202572 | Val Loss: 0.194820\n",
      "2025-05-10 12:15:55,253 - INFO - Epoch 48/100 | Train Loss: 0.201626 | Val Loss: 0.197615\n",
      "2025-05-10 12:15:55,504 - INFO - Epoch 49/100 | Train Loss: 0.199217 | Val Loss: 0.216214\n",
      "2025-05-10 12:15:55,747 - INFO - Epoch 50/100 | Train Loss: 0.198889 | Val Loss: 0.175423\n",
      "2025-05-10 12:15:55,985 - INFO - Epoch 51/100 | Train Loss: 0.194091 | Val Loss: 0.197767\n",
      "2025-05-10 12:15:56,253 - INFO - Epoch 52/100 | Train Loss: 0.196572 | Val Loss: 0.186095\n",
      "2025-05-10 12:15:56,495 - INFO - Epoch 53/100 | Train Loss: 0.186821 | Val Loss: 0.192787\n",
      "2025-05-10 12:15:56,743 - INFO - Epoch 54/100 | Train Loss: 0.191829 | Val Loss: 0.191114\n",
      "2025-05-10 12:15:56,981 - INFO - Epoch 55/100 | Train Loss: 0.191772 | Val Loss: 0.186373\n",
      "2025-05-10 12:15:57,213 - INFO - Epoch 56/100 | Train Loss: 0.185283 | Val Loss: 0.209196\n",
      "2025-05-10 12:15:57,432 - INFO - Epoch 57/100 | Train Loss: 0.192272 | Val Loss: 0.190355\n",
      "2025-05-10 12:15:57,656 - INFO - Epoch 58/100 | Train Loss: 0.195895 | Val Loss: 0.196804\n",
      "2025-05-10 12:15:57,902 - INFO - Epoch 59/100 | Train Loss: 0.188585 | Val Loss: 0.181294\n",
      "2025-05-10 12:15:58,173 - INFO - Epoch 60/100 | Train Loss: 0.191540 | Val Loss: 0.184663\n",
      "2025-05-10 12:15:58,174 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:15:58,175 - INFO - Training with params: {'batch_size': 32, 'latent_dim': 32, 'learning_rate': 0.0005, 'window_size': 96}\n",
      "2025-05-10 12:15:58,472 - INFO - Epoch 1/100 | Train Loss: 0.992703 | Val Loss: 0.969515\n",
      "2025-05-10 12:15:58,754 - INFO - Epoch 2/100 | Train Loss: 0.940662 | Val Loss: 0.897835\n",
      "2025-05-10 12:15:59,001 - INFO - Epoch 3/100 | Train Loss: 0.850070 | Val Loss: 0.804671\n",
      "2025-05-10 12:15:59,264 - INFO - Epoch 4/100 | Train Loss: 0.770118 | Val Loss: 0.750949\n",
      "2025-05-10 12:15:59,532 - INFO - Epoch 5/100 | Train Loss: 0.717050 | Val Loss: 0.685585\n",
      "2025-05-10 12:15:59,784 - INFO - Epoch 6/100 | Train Loss: 0.675010 | Val Loss: 0.647969\n",
      "2025-05-10 12:16:00,019 - INFO - Epoch 7/100 | Train Loss: 0.637648 | Val Loss: 0.630415\n",
      "2025-05-10 12:16:00,341 - INFO - Epoch 8/100 | Train Loss: 0.611965 | Val Loss: 0.607516\n",
      "2025-05-10 12:16:00,672 - INFO - Epoch 9/100 | Train Loss: 0.588803 | Val Loss: 0.570149\n",
      "2025-05-10 12:16:01,044 - INFO - Epoch 10/100 | Train Loss: 0.570891 | Val Loss: 0.558098\n",
      "2025-05-10 12:16:01,319 - INFO - Epoch 11/100 | Train Loss: 0.557531 | Val Loss: 0.559438\n",
      "2025-05-10 12:16:01,570 - INFO - Epoch 12/100 | Train Loss: 0.543846 | Val Loss: 0.539397\n",
      "2025-05-10 12:16:01,825 - INFO - Epoch 13/100 | Train Loss: 0.530461 | Val Loss: 0.518630\n",
      "2025-05-10 12:16:02,075 - INFO - Epoch 14/100 | Train Loss: 0.523620 | Val Loss: 0.517506\n",
      "2025-05-10 12:16:02,306 - INFO - Epoch 15/100 | Train Loss: 0.513447 | Val Loss: 0.507064\n",
      "2025-05-10 12:16:02,563 - INFO - Epoch 16/100 | Train Loss: 0.497857 | Val Loss: 0.494528\n",
      "2025-05-10 12:16:02,819 - INFO - Epoch 17/100 | Train Loss: 0.495004 | Val Loss: 0.494000\n",
      "2025-05-10 12:16:03,120 - INFO - Epoch 18/100 | Train Loss: 0.494827 | Val Loss: 0.484330\n",
      "2025-05-10 12:16:03,378 - INFO - Epoch 19/100 | Train Loss: 0.481817 | Val Loss: 0.478116\n",
      "2025-05-10 12:16:03,628 - INFO - Epoch 20/100 | Train Loss: 0.473784 | Val Loss: 0.472844\n",
      "2025-05-10 12:16:03,875 - INFO - Epoch 21/100 | Train Loss: 0.475036 | Val Loss: 0.468246\n",
      "2025-05-10 12:16:04,117 - INFO - Epoch 22/100 | Train Loss: 0.470159 | Val Loss: 0.464270\n",
      "2025-05-10 12:16:04,362 - INFO - Epoch 23/100 | Train Loss: 0.463726 | Val Loss: 0.469564\n",
      "2025-05-10 12:16:04,592 - INFO - Epoch 24/100 | Train Loss: 0.460185 | Val Loss: 0.466317\n",
      "2025-05-10 12:16:04,840 - INFO - Epoch 25/100 | Train Loss: 0.461689 | Val Loss: 0.459905\n",
      "2025-05-10 12:16:05,064 - INFO - Epoch 26/100 | Train Loss: 0.455899 | Val Loss: 0.451774\n",
      "2025-05-10 12:16:05,299 - INFO - Epoch 27/100 | Train Loss: 0.450188 | Val Loss: 0.454346\n",
      "2025-05-10 12:16:05,569 - INFO - Epoch 28/100 | Train Loss: 0.455203 | Val Loss: 0.442810\n",
      "2025-05-10 12:16:05,817 - INFO - Epoch 29/100 | Train Loss: 0.453761 | Val Loss: 0.446796\n",
      "2025-05-10 12:16:06,087 - INFO - Epoch 30/100 | Train Loss: 0.443711 | Val Loss: 0.437896\n",
      "2025-05-10 12:16:06,323 - INFO - Epoch 31/100 | Train Loss: 0.446449 | Val Loss: 0.443661\n",
      "2025-05-10 12:16:06,549 - INFO - Epoch 32/100 | Train Loss: 0.442558 | Val Loss: 0.459767\n",
      "2025-05-10 12:16:06,819 - INFO - Epoch 33/100 | Train Loss: 0.440927 | Val Loss: 0.443931\n",
      "2025-05-10 12:16:07,052 - INFO - Epoch 34/100 | Train Loss: 0.440985 | Val Loss: 0.437977\n",
      "2025-05-10 12:16:07,295 - INFO - Epoch 35/100 | Train Loss: 0.443945 | Val Loss: 0.442251\n",
      "2025-05-10 12:16:07,519 - INFO - Epoch 36/100 | Train Loss: 0.431040 | Val Loss: 0.424444\n",
      "2025-05-10 12:16:07,776 - INFO - Epoch 37/100 | Train Loss: 0.435009 | Val Loss: 0.436186\n",
      "2025-05-10 12:16:08,061 - INFO - Epoch 38/100 | Train Loss: 0.429954 | Val Loss: 0.437501\n",
      "2025-05-10 12:16:08,327 - INFO - Epoch 39/100 | Train Loss: 0.429222 | Val Loss: 0.435643\n",
      "2025-05-10 12:16:08,542 - INFO - Epoch 40/100 | Train Loss: 0.430069 | Val Loss: 0.415904\n",
      "2025-05-10 12:16:08,800 - INFO - Epoch 41/100 | Train Loss: 0.428280 | Val Loss: 0.435853\n",
      "2025-05-10 12:16:09,031 - INFO - Epoch 42/100 | Train Loss: 0.429301 | Val Loss: 0.429247\n",
      "2025-05-10 12:16:09,286 - INFO - Epoch 43/100 | Train Loss: 0.435201 | Val Loss: 0.434855\n",
      "2025-05-10 12:16:09,515 - INFO - Epoch 44/100 | Train Loss: 0.429719 | Val Loss: 0.438779\n",
      "2025-05-10 12:16:09,766 - INFO - Epoch 45/100 | Train Loss: 0.436816 | Val Loss: 0.427835\n",
      "2025-05-10 12:16:10,024 - INFO - Epoch 46/100 | Train Loss: 0.425463 | Val Loss: 0.436868\n",
      "2025-05-10 12:16:10,265 - INFO - Epoch 47/100 | Train Loss: 0.426362 | Val Loss: 0.438000\n",
      "2025-05-10 12:16:10,495 - INFO - Epoch 48/100 | Train Loss: 0.434031 | Val Loss: 0.429306\n",
      "2025-05-10 12:16:10,745 - INFO - Epoch 49/100 | Train Loss: 0.429666 | Val Loss: 0.431747\n",
      "2025-05-10 12:16:11,008 - INFO - Epoch 50/100 | Train Loss: 0.424923 | Val Loss: 0.429316\n",
      "2025-05-10 12:16:11,008 - INFO - Early stopping triggered.\n",
      "2025-05-10 12:16:11,010 - INFO - Best params: {'batch_size': 32, 'latent_dim': 16, 'learning_rate': 0.0005, 'window_size': 64} with Val Loss: 0.154934\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FEATHER_FILE = 'merged_data_cell_a_pdu6_pdu7_approx100machines_30d.feather'\n",
    "TARGET_PDU = 'pdu6'\n",
    "FEATURE_COLUMNS = ['pdu_sum_cpu_usage', 'production_power_util']\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "PARAM_GRID = {\n",
    "    'window_size': [64, 96],\n",
    "    'latent_dim': [16, 32],\n",
    "    'learning_rate': [1e-3, 5e-4],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "NOISE_STEPS = 1000\n",
    "BETA_START, BETA_END = 1e-4, 0.02\n",
    "OUTPUT_DIR = 'diffusion_model_output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset for time series windows\n",
    "# -----------------------------------------------------------------------------\n",
    "class TimeSeriesWindowDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, window_size: int):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x0 = self.data[idx:idx + self.window_size]\n",
    "        return x0.astype(np.float32)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Time embedding for diffusion steps\n",
    "# -----------------------------------------------------------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        return emb\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Simple MLP denoiser model\n",
    "# -----------------------------------------------------------------------------\n",
    "class Denoiser(nn.Module):\n",
    "    def __init__(self, window_size, feature_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        input_dim = window_size * feature_dim + latent_dim\n",
    "        hidden = 128\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, window_size * feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_noisy, t_emb):\n",
    "        # x_noisy: (batch, window_size*feature_dim)\n",
    "        # t_emb:   (batch, latent_dim)\n",
    "        h = torch.cat([x_noisy, t_emb], dim=1)\n",
    "        out = self.net(h)\n",
    "        return out\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Diffusion model training and evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "class TimeSeriesDiffusionModel:\n",
    "    def __init__(self, window_size, feature_dim, latent_dim, lr, batch_size, device):\n",
    "        self.window_size = window_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Beta schedule\n",
    "        self.betas = torch.linspace(BETA_START, BETA_END, NOISE_STEPS).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # Model and optimizer\n",
    "        self.model = Denoiser(window_size, feature_dim, latent_dim).to(device)\n",
    "        self.pos_emb = SinusoidalPosEmb(latent_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def q_sample(self, x0, t, noise):\n",
    "        # x0: (batch, window_size*feature_dim)\n",
    "        sqrt_alphas_cumprod = self.alphas_cumprod[t]**0.5\n",
    "        sqrt_one_minus_alphas = (1 - self.alphas_cumprod[t])**0.5\n",
    "        return sqrt_alphas_cumprod[:, None] * x0 + sqrt_one_minus_alphas[:, None] * noise\n",
    "\n",
    "    def p_loss(self, x0):\n",
    "        batch_size = x0.size(0)\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, NOISE_STEPS, (batch_size,), device=self.device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        x_noisy = self.q_sample(x0, t, noise)\n",
    "        t_emb = self.pos_emb(t)\n",
    "        pred_noise = self.model(x_noisy, t_emb)\n",
    "        loss = nn.functional.mse_loss(pred_noise, noise, reduction='mean')\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        best_val = float('inf')\n",
    "        patience_counter = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "            for windows in train_loader:\n",
    "                x0 = windows.view(-1, self.window_size * self.feature_dim).to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.p_loss(x0)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "            avg_train = np.mean(train_losses)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for windows in val_loader:\n",
    "                    x0 = windows.view(-1, self.window_size * self.feature_dim).to(self.device)\n",
    "                    val_losses.append(self.p_loss(x0).item())\n",
    "            avg_val = np.mean(val_losses)\n",
    "\n",
    "            logging.info(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train:.6f} | Val Loss: {avg_val:.6f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val < best_val:\n",
    "                best_val = avg_val\n",
    "                torch.save(self.model.state_dict(), os.path.join(OUTPUT_DIR, f\"best_diffusion_w{self.window_size}_ld{self.latent_dim}_lr{self.lr}.pth\"))\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    logging.info(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        return best_val\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Generate synthetic sequences using reverse diffusion.\"\"\"\n",
    "        self.model.eval()\n",
    "        x = torch.randn(num_samples, self.window_size * self.feature_dim).to(self.device)\n",
    "\n",
    "        for t in reversed(range(len(self.betas))):  # NOISE_STEPS\n",
    "            t_tensor = torch.full((num_samples,), t, device=self.device, dtype=torch.long)\n",
    "            t_emb = self.pos_emb(t_tensor)\n",
    "            pred_noise = self.model(x, t_emb)\n",
    "\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_hat = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x)\n",
    "\n",
    "            x = (1 / alpha**0.5) * (x - ((1 - alpha) / (1 - alpha_hat)**0.5) * pred_noise) + (beta**0.5) * noise\n",
    "\n",
    "        return x.view(-1, self.window_size, self.feature_dim)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main execution with grid search\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load and filter data\n",
    "    df = pd.read_feather(INPUT_FEATHER_FILE)\n",
    "    df = df[df['pdu'] == TARGET_PDU].copy()\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    data = df[FEATURE_COLUMNS].values\n",
    "\n",
    "    # 2. Normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    results = []\n",
    "    for params in ParameterGrid(PARAM_GRID):\n",
    "        logging.info(f\"Training with params: {params}\")\n",
    "        # Prepare windows\n",
    "        dataset = TimeSeriesWindowDataset(data_scaled, params['window_size'])\n",
    "        train_size = int(len(dataset) * 0.8)\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        # Initialize model\n",
    "        model = TimeSeriesDiffusionModel(\n",
    "            window_size=params['window_size'],\n",
    "            feature_dim=len(FEATURE_COLUMNS),\n",
    "            latent_dim=params['latent_dim'],\n",
    "            lr=params['learning_rate'],\n",
    "            batch_size=params['batch_size'],\n",
    "            device=device\n",
    "        )\n",
    "        # Train\n",
    "        val_loss = model.train(train_loader, val_loader)\n",
    "        results.append((params, val_loss))\n",
    "\n",
    "    # Find best\n",
    "    best_params, best_loss = sorted(results, key=lambda x: x[1])[0]\n",
    "    logging.info(f\"Best params: {best_params} with Val Loss: {best_loss:.6f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
